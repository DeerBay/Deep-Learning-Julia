{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNyW85j4rXhKria458LZz+P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeerBay/Deep-Learning-Julia/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium['atari, other'] gymnasium ale-py tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_oJLHLqkPfSt",
        "outputId": "76340e97-c012-4223-d09c-289190d1e9da"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting ale-py\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari,other]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari,other]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari,other]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[atari,other])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Requirement already satisfied: moviepy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari,other]) (1.0.3)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari,other]) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari,other]) (4.10.0.84)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[atari,other]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[atari,other]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[atari,other]) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[atari,other]) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[atari,other]) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[atari,other]) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[atari,other]) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[atari,other]) (2.8.2)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[atari,other]) (4.4.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[atari,other]) (2.36.1)\n",
            "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[atari,other]) (0.5.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[atari,other]) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[atari,other]) (0.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio_ffmpeg>=0.2.0->moviepy>=1.0.0->gymnasium[atari,other]) (75.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[atari,other]) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[atari,other]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[atari,other]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[atari,other]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[atari,other]) (2024.8.30)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py\n",
            "Successfully installed ale-py-0.10.1 farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "collapsed": true,
        "id": "E96bASUZPUcw",
        "outputId": "3075f0ef-9a9a-4a6b-857e-13174aa87f85"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c1f81b1d8a33>\u001b[0m in \u001b[0;36m<cell line: 206>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-c1f81b1d8a33>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# Store in replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# Update state and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-c1f81b1d8a33>\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import FrameStackObservation, AtariPreprocessing, RecordVideo\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Performance and GPU Configuration\n",
        "tf.config.optimizer.set_jit(True)\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size, state_shape, num_actions):\n",
        "        self.max_size = max_size\n",
        "        self.states = np.zeros((max_size, *state_shape), dtype=np.float32)\n",
        "        self.actions = np.zeros(max_size, dtype=np.int32)\n",
        "        self.rewards = np.zeros(max_size, dtype=np.float32)\n",
        "        self.next_states = np.zeros((max_size, *state_shape), dtype=np.float32)\n",
        "        self.dones = np.zeros(max_size, dtype=np.float32)\n",
        "        self.index = 0\n",
        "        self.is_full = False\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        idx = self.index % self.max_size\n",
        "        self.states[idx] = state\n",
        "        self.actions[idx] = action\n",
        "        self.rewards[idx] = reward\n",
        "        self.next_states[idx] = next_state\n",
        "        self.dones[idx] = done\n",
        "        self.index += 1\n",
        "        if self.index >= self.max_size:\n",
        "            self.is_full = True\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        max_index = self.max_size if self.is_full else self.index\n",
        "        indices = np.random.choice(max_index, batch_size, replace=False)\n",
        "        return (\n",
        "            self.states[indices],\n",
        "            self.actions[indices],\n",
        "            self.rewards[indices],\n",
        "            self.next_states[indices],\n",
        "            self.dones[indices]\n",
        "        )\n",
        "    def __len__(self):  # This ensures len(replay_buffer) works\n",
        "        return self.max_size if self.is_full else self.index\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_shape, num_actions, learning_rate=0.00025):\n",
        "        self.state_shape = state_shape\n",
        "        self.num_actions = num_actions\n",
        "        self.model = self.create_q_model(state_shape, num_actions)\n",
        "        self.target_model = self.create_q_model(state_shape, num_actions)\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
        "        self.replay_buffer = ReplayBuffer(max_size=100000, state_shape=state_shape, num_actions=num_actions)\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.1\n",
        "        self.epsilon_decay = 0.99995\n",
        "\n",
        "    def create_q_model(self, input_shape, num_actions):\n",
        "        return tf.keras.Sequential([\n",
        "            layers.Input(shape=input_shape),\n",
        "            layers.Conv2D(32, kernel_size=8, strides=4, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "            layers.Conv2D(64, kernel_size=4, strides=2, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "            layers.Conv2D(64, kernel_size=3, strides=1, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(512, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "            layers.Dense(num_actions, activation=\"linear\")\n",
        "        ])\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.randint(self.num_actions)\n",
        "        state_tensor = tf.convert_to_tensor(state[np.newaxis, ...], dtype=tf.float32)\n",
        "        q_values = self.model(state_tensor, training=False)\n",
        "        return tf.argmax(q_values[0]).numpy()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, states, actions, rewards, next_states, dones):\n",
        "        future_rewards = self.target_model(next_states, training=False)\n",
        "        target_q_values = rewards + self.gamma * tf.reduce_max(future_rewards, axis=1) * (1 - dones)\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = self.model(states, training=True)\n",
        "            q_action = tf.reduce_sum(tf.one_hot(actions, self.num_actions) * q_values, axis=1)\n",
        "            loss = tf.keras.losses.Huber()(target_q_values, q_action)\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "def main():\n",
        "    # Environment setup\n",
        "    env = gym.make(\"SpaceInvadersNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
        "    env = AtariPreprocessing(env, frame_skip=4, grayscale_obs=True, scale_obs=True)\n",
        "    env = FrameStackObservation(env, stack_size=4)\n",
        "    video_folder = \"videos\"\n",
        "    os.makedirs(video_folder, exist_ok=True)\n",
        "\n",
        "    # Agent setup\n",
        "    state_shape = (84, 84, 4)\n",
        "    num_actions = env.action_space.n\n",
        "    agent = DQNAgent(state_shape, num_actions)\n",
        "\n",
        "    # Training parameters\n",
        "    max_episodes = 5000\n",
        "    max_steps_per_episode = 10000\n",
        "    update_target_every = 1000\n",
        "    log_every = 100\n",
        "    epsilon_random_frames = 50000  # Number of frames to take random actions and observe output\n",
        "    episode_rewards = []\n",
        "    frame_count = 0\n",
        "\n",
        "    # Open a file for logging results\n",
        "    log_file = \"training_log.txt\"\n",
        "    with open(log_file, \"w\") as f:\n",
        "        f.write(\"Episode,Avg_Reward,Epsilon\\n\")  # Write headers\n",
        "\n",
        "    # Training loop\n",
        "    for episode in range(max_episodes):\n",
        "        state, _ = env.reset()\n",
        "        state = np.array(state, dtype=np.float32) / 255.0\n",
        "        state = np.transpose(state, (1, 2, 0))\n",
        "        episode_reward = 0\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            frame_count += 1\n",
        "\n",
        "            # Choose action\n",
        "            if frame_count < epsilon_random_frames:\n",
        "                # Take random action during the initial exploration phase\n",
        "                action = np.random.randint(num_actions)\n",
        "            else:\n",
        "                # Use epsilon-greedy policy after the exploration phase\n",
        "                action = agent.get_action(state)\n",
        "\n",
        "            # Step environment\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            next_state = np.array(next_state, dtype=np.float32) / 255.0\n",
        "            next_state = np.transpose(next_state, (1, 2, 0))\n",
        "\n",
        "            # Store in replay buffer\n",
        "            agent.replay_buffer.add(state, action, reward, next_state, done)\n",
        "\n",
        "            # Update state and reward\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            # Train only if enough samples in replay buffer\n",
        "            if len(agent.replay_buffer) >= 32:\n",
        "                states, actions, rewards, next_states, dones = agent.replay_buffer.sample(32)\n",
        "                agent.train_step(states, actions, rewards, next_states, dones)\n",
        "\n",
        "            # Decay epsilon (only after the random exploration phase)\n",
        "            if frame_count >= epsilon_random_frames:\n",
        "                agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
        "\n",
        "            # Update target network periodically\n",
        "            if frame_count % update_target_every == 0:\n",
        "                agent.update_target_model()\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Logging results\n",
        "        episode_rewards.append(episode_reward)\n",
        "        if (episode + 1) % log_every == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-log_every:])\n",
        "            with open(log_file, \"a\") as f:\n",
        "                f.write(f\"{episode + 1},{avg_reward:.2f},{agent.epsilon:.4f}\\n\")\n",
        "                f.flush()\n",
        "            print(f\"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}, Epsilon: {agent.epsilon:.4f}\")\n",
        "\n",
        "        # Save video every 100 episodes\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            os.makedirs('models', exist_ok=True)\n",
        "            agent.model.save(f'models/episode{episode + 1}_space_invaders_model.keras')\n",
        "            video_env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda ep: True)\n",
        "            video_state, _ = video_env.reset()\n",
        "            video_state = np.array(video_state, dtype=np.float32) / 255.0\n",
        "            video_state = np.transpose(video_state, (1, 2, 0))\n",
        "            for _ in range(max_steps_per_episode):\n",
        "                video_action = agent.get_action(video_state)\n",
        "                video_next_state, _, video_done, _, _ = video_env.step(video_action)\n",
        "                video_state = np.array(video_next_state, dtype=np.float32) / 255.0\n",
        "                video_state = np.transpose(video_state, (1, 2, 0))\n",
        "                if video_done:\n",
        "                    break\n",
        "            video_env.close()\n",
        "\n",
        "    # Save final model\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "    agent.model.save('models/final_space_invaders_model.keras')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}