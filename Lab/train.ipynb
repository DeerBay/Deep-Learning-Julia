{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FrameStackObservation, AtariPreprocessing, RecordVideo\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import os\n",
    "import ale_py\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/CPU:0'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device selection\n",
    "device_name = \"gpu\"  # Set to \"cpu\" or \"gpu\" as needed\n",
    "if device_name == \"gpu\" and tf.config.list_physical_devices('GPU'):\n",
    "    device = \"/GPU:0\"\n",
    "else:\n",
    "    device = \"/CPU:0\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "gym.register_envs(ale_py)\n",
    "env = gym.make(\"SpaceInvadersNoFrameskip-v4\", render_mode=\"rgb_array\", frameskip=1) \n",
    "env = AtariPreprocessing(env)\n",
    "env = FrameStackObservation(env, 4)\n",
    "trigger = lambda t: t % 20 == 0 # Every 20th episode\n",
    "env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 6\n",
      "Actions and their meanings: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# Get the number of actions\n",
    "num_actions = env.action_space.n\n",
    "print(f\"Number of actions: {num_actions}\")\n",
    "# List all action meanings\n",
    "action_meanings = env.unwrapped.get_action_meanings()\n",
    "print(\"Actions and their meanings:\", action_meanings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "seed = 42\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 1.0\n",
    "epsilon_interval = (epsilon_max - epsilon_min)\n",
    "epsilon_decay_frames = 1000000  # Adjust this value if needed\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "max_steps_per_episode = 10000\n",
    "max_episodes = 0 # Update this\n",
    "max_frames = 1e7\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon_random_frames = 50000\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "\n",
    "# Replay buffer parameters\n",
    "# NOTE: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000\n",
    "update_after_actions = 4\n",
    "update_target_network = 10000\n",
    "\n",
    "# Using huber loss for stability (specifically for Adam)\n",
    "loss_function = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize history variables\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_q_model():\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(84, 84, 4)),\n",
    "            layers.Conv2D(32, kernel_size=8, strides=4, activation=\"relu\"),\n",
    "            layers.Conv2D(64, kernel_size=4, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(64, kernel_size=3, strides=1, activation=\"relu\"),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(512, activation=\"relu\"),\n",
    "            layers.Dense(num_actions, activation=\"linear\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "model = create_q_model()\n",
    "model_target = create_q_model()\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Create directory for saving models\n",
    "save_dir = \"models\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 0episode [02:44, ?episode/s]\n",
      "\n",
      "Frames:   0%|          | 0/10000000.0 [02:44<?, ?frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: /CPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved: models/best_model_episode_1.keras with running reward: 475.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Convert to tensors\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m state_sample \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Combine into a single tensor\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m#print(\"state_sample shape before transpose:\", state_sample.shape)\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Transpose to (batch_size, 84, 84, 4)\u001b[39;00m\n\u001b[0;32m     73\u001b[0m state_sample \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtranspose(state_sample, perm\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Correct permutation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\glajusj\\Documents\\github\\Deep-Learning-Julia\\.env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\glajusj\\Documents\\github\\Deep-Learning-Julia\\.env\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\glajusj\\Documents\\github\\Deep-Learning-Julia\\.env\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops_stack.py:74\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     72\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# If the input is a constant list, it can be converted to a constant op\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Input list contains non-constant tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\glajusj\\Documents\\github\\Deep-Learning-Julia\\.env\\Lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\glajusj\\Documents\\github\\Deep-Learning-Julia\\.env\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:732\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[0;32m    731\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[1;32m--> 732\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\glajusj\\Documents\\github\\Deep-Learning-Julia\\.env\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:240\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# Convert ret to Tensor if it is a core.Tensor type.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, core\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 240\u001b[0m   to_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__tf_tensor__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m   ret \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    242\u001b[0m       to_tensor()  \u001b[38;5;66;03m#  pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    243\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m to_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    244\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m ret\n\u001b[0;32m    245\u001b[0m   )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, accepted_result_types):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Replay buffer\n",
    "replay_buffer = deque(maxlen=max_memory_length)\n",
    "\n",
    "# Initialize variables\n",
    "best_running_reward = -float('inf')  # Start with the lowest possible reward\n",
    "last_saved_model = None  # Track the last saved model\n",
    "log_file = \"model_performance_log.txt\"  # Log file path\n",
    "\n",
    "# Initialize tqdm progress bars\n",
    "total_episodes = max_episodes if max_episodes > 0 else float('inf')  # Total episodes, if max_episodes is set\n",
    "total_frames = max_frames if max_frames > 0 else float('inf')  # Total frames, if max_frames is set\n",
    "\n",
    "episode_bar = tqdm(total=total_episodes, desc=\"Episodes\", unit=\"episode\")\n",
    "frame_bar = tqdm(total=total_frames, desc=\"Frames\", unit=\"frame\")\n",
    "\n",
    "# Clear log file\n",
    "with open(log_file, \"w\") as f:\n",
    "    f.write(\"Episode\\tRunning Reward\\tEpsilon\\n\")\n",
    "\n",
    "# Initialize history\n",
    "episode_reward_history = deque(maxlen=100)  # Efficient rolling window\n",
    "running_reward = 0\n",
    "\n",
    "# Target network update tau for soft updates\n",
    "tau = 0.001\n",
    "\n",
    "with tf.device(device):  # Ensures all TensorFlow operations are executed on the selected device\n",
    "    print(f\"Using device: {device}\")\n",
    "    while True:\n",
    "        # Reset environment\n",
    "        observation, _ = env.reset()\n",
    "        state = np.array(observation)  # Preprocessed state\n",
    "        #print(\"State shape:\", state.shape)  # Debugging to verify shape\n",
    "        episode_reward = 0\n",
    "\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            frame_count += 1\n",
    "\n",
    "            # Epsilon-greedy action selection\n",
    "            if frame_count < epsilon_random_frames or tf.random.uniform((1,)) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Ensure state is in the correct format: (batch_size, height, width, channels)\n",
    "                state_tensor = keras.ops.convert_to_tensor(state)\n",
    "                state_tensor = keras.ops.expand_dims(state_tensor, 0)\n",
    "                action_probs = model(state_tensor, training=False)\n",
    "                action = keras.ops.argmax(action_probs[0]).numpy()\n",
    "\n",
    "            # Decay probability of taking random action\n",
    "            epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "            epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "            # Step in the environment\n",
    "            state_next, reward, done, _, _ = env.step(action)\n",
    "            state_next = np.array(state_next)  # Preprocessed next state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Store transition in replay buffer\n",
    "            replay_buffer.append((state, action, reward, state_next, done))\n",
    "            state = state_next  # Update state\n",
    "\n",
    "            # Train the model\n",
    "            if frame_count % update_after_actions == 0 and len(replay_buffer) >= batch_size:\n",
    "                # Sample a batch from replay buffer\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                # Convert to tensors\n",
    "                state_sample = tf.stack(states, axis=0)  # Combine into a single tensor\n",
    "                #print(\"state_sample shape before transpose:\", state_sample.shape)\n",
    "\n",
    "                # Transpose to (batch_size, 84, 84, 4)\n",
    "                state_sample = tf.transpose(state_sample, perm=[0, 2, 3, 1])  # Correct permutation\n",
    "                #print(\"state_sample shape after transpose:\", state_sample.shape)\n",
    "\n",
    "                state_sample = tf.cast(state_sample, dtype=tf.float32)\n",
    "                \n",
    "\n",
    "                next_state_sample = tf.stack(next_states, axis=0)  # Result: (batch_size, 4, 84, 84)\n",
    "                #print(\"next_state_sample shape before transpose:\", next_state_sample.shape)\n",
    "                next_state_sample = tf.transpose(next_state_sample, perm=[0, 2, 3, 1])  # Rearrange to (batch_size, 84, 84, 4)\n",
    "                #print(\"next_state_sample shape after transpose:\", next_state_sample.shape)\n",
    "                next_state_sample = tf.cast(next_state_sample, dtype=tf.float32)\n",
    "\n",
    "                action_sample = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "                reward_sample = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "                done_sample = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "                # Debugging final shapes before passing to the model\n",
    "                #print(\"state_sample final shape:\", state_sample.shape)  # Expected: (batch_size, 84, 84, 4)\n",
    "                #print(\"next_state_sample final shape:\", next_state_sample.shape)  # Expected: (batch_size, 84, 84, 4)\n",
    "                #print(\"action_sample final shape:\", action_sample.shape)  # Expected: (batch_size,)\n",
    "\n",
    "                # Compute target Q-values\n",
    "                future_rewards = model_target(next_state_sample)\n",
    "                updated_q_values = reward_sample + gamma * tf.reduce_max(future_rewards, axis=1) * (1 - done_sample)\n",
    "\n",
    "                # Masked loss\n",
    "                with tf.GradientTape() as tape:\n",
    "                    q_values = model(state_sample)\n",
    "                    q_action = tf.reduce_sum(tf.one_hot(action_sample, num_actions) * q_values, axis=1)\n",
    "                    loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "                # Backpropagation\n",
    "                grads = tape.gradient(loss, model.trainable_variables)\n",
    "                #print(\"Gradient shapes:\", [g.shape for g in grads if g is not None])\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            # Update target network\n",
    "            if frame_count % update_target_network == 0:\n",
    "                model_target.set_weights([\n",
    "                    tau * w + (1 - tau) * tw\n",
    "                    for w, tw in zip(model.get_weights(), model_target.get_weights())\n",
    "                ])\n",
    "\n",
    "            # End episode if done\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update running reward and history\n",
    "        episode_reward_history.append(episode_reward)\n",
    "        running_reward = np.mean(episode_reward_history)\n",
    "        episode_count += 1\n",
    "        # Update progress bar\n",
    "        episode_bar.update(1)  # Increment the progress bar by one episode\n",
    "        episode_bar.set_postfix(running_reward=f\"{running_reward:.2f}\", epsilon=f\"{epsilon:.2f}\")\n",
    "\n",
    "        # Check if the model should be saved\n",
    "        if running_reward > best_running_reward:\n",
    "            best_running_reward = running_reward\n",
    "\n",
    "            # Remove the last saved model if it exists\n",
    "            if last_saved_model and os.path.exists(last_saved_model):\n",
    "                os.remove(last_saved_model)\n",
    "\n",
    "            # Save the new best model\n",
    "            model_path = f\"models/best_model_episode_{episode_count}.keras\"\n",
    "            model.save(model_path)\n",
    "            last_saved_model = model_path\n",
    "\n",
    "            # Log the result\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"{episode_count}\\t{running_reward:.2f}\\t{epsilon:.2f}\\n\")\n",
    "\n",
    "            print(f\"New best model saved: {model_path} with running reward: {running_reward:.2f}\")\n",
    "\n",
    "        # Print progress\n",
    "        if episode_count % 10 == 0:\n",
    "            print(f\"Episode {episode_count}, Frame {frame_count}, Running Reward: {running_reward:.2f}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "        # Termination conditions\n",
    "        if running_reward > 10000:\n",
    "            print(f\"Solved with a running_reward of {running_reward} at episode {episode_count}!\")\n",
    "            model.save(\"spaceinvaders_qmodel_solved.keras\")\n",
    "            break\n",
    "\n",
    "        if max_episodes > 0 and episode_count >= max_episodes:\n",
    "            print(f\"Stopped at episode {episode_count}!\")\n",
    "            break\n",
    "\n",
    "        if max_frames > 0 and frame_count >= max_frames:\n",
    "            print(f\"Stopped at frame {frame_count}!\")\n",
    "            break\n",
    "\n",
    "    # Final save after training\n",
    "    model.save(\"spaceinvaders_qmodel_final.keras\")\n",
    "    print(\"Final model saved as 'spaceinvaders_qmodel_final.keras'.\")\n",
    "    print(\"Training complete.\")\n",
    "episode_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\glajusj\\Documents\\github\\Deep-Learning-Julia\\.env\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:416: UserWarning: \u001b[33mWARN: Unable to save last video! Did you call close()?\u001b[0m\n",
      "  logger.warn(\"Unable to save last video! Did you call close()?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 6\n",
      "Actions and their meanings: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
      "New best model saved: models/best_model_episode_1.keras with running reward: 240.00\n",
      "New best model saved: models/best_model_episode_2.keras with running reward: 325.00\n",
      "Episode 10, Frame 5561, Running Reward: 181.00, Epsilon: 1.00\n",
      "Episode 20, Frame 9903, Running Reward: 143.00, Epsilon: 0.99\n",
      "Episode 30, Frame 14455, Running Reward: 129.17, Epsilon: 0.99\n",
      "Episode 40, Frame 19871, Running Reward: 132.25, Epsilon: 0.98\n",
      "Episode 50, Frame 24699, Running Reward: 132.30, Epsilon: 0.98\n",
      "Episode 60, Frame 30027, Running Reward: 132.83, Epsilon: 0.97\n",
      "Episode 70, Frame 35091, Running Reward: 133.93, Epsilon: 0.97\n",
      "Episode 80, Frame 41215, Running Reward: 139.44, Epsilon: 0.96\n",
      "Episode 90, Frame 46330, Running Reward: 139.33, Epsilon: 0.96\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__ConcatV2_N_32_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 2 in both shapes must be equal: shape[0] = [1,84,84,4] vs. shape[4] = [1,84,4,84] [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 134\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Ensure all tensors are in correct shape\u001b[39;00m\n\u001b[0;32m    133\u001b[0m state_sample \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mtranspose(s, perm\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states]\n\u001b[1;32m--> 134\u001b[0m state_sample \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m state_sample \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(state_sample, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    137\u001b[0m next_state_sample \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mtranspose(s, perm\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m next_states]\n",
      "File \u001b[1;32mc:\\Users\\glajusj\\Documents\\github\\Deep-Learning-Julia\\.env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\glajusj\\Documents\\github\\Deep-Learning-Julia\\.env\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6002\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   6001\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 6002\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ConcatV2_N_32_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 2 in both shapes must be equal: shape[0] = [1,84,84,4] vs. shape[4] = [1,84,4,84] [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "# # Environment setup\n",
    "# gym.register_envs(ale_py)\n",
    "# env = gym.make(\"SpaceInvadersNoFrameskip-v4\", render_mode=\"rgb_array\", frameskip=1)\n",
    "# env = AtariPreprocessing(env)\n",
    "# env = FrameStackObservation(env, 4)\n",
    "# trigger = lambda t: t % 20 == 0  # Every 20th episode\n",
    "# env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)\n",
    "\n",
    "# # Get the number of actions\n",
    "# num_actions = env.action_space.n\n",
    "# print(f\"Number of actions: {num_actions}\")\n",
    "# # List all action meanings\n",
    "# action_meanings = env.unwrapped.get_action_meanings()\n",
    "# print(\"Actions and their meanings:\", action_meanings)\n",
    "\n",
    "# # Hyper parameters\n",
    "# seed = 42\n",
    "# gamma = 0.99\n",
    "# epsilon = 1.0\n",
    "# epsilon_min = 0.1\n",
    "# epsilon_max = 1.0\n",
    "# epsilon_interval = (epsilon_max - epsilon_min)\n",
    "# epsilon_decay_frames = 1000000  # Adjust this value if needed\n",
    "\n",
    "# batch_size = 32\n",
    "# max_steps_per_episode = 10000\n",
    "# max_episodes = 0  # Update this\n",
    "# max_frames = 1e7\n",
    "\n",
    "# # Exploration parameters\n",
    "# epsilon_random_frames = 50000\n",
    "# epsilon_greedy_frames = 1000000.0\n",
    "\n",
    "# # Replay buffer parameters\n",
    "# # NOTE: The Deepmind paper suggests 1e6 however this causes memory issues\n",
    "# max_memory_length = 100000\n",
    "# update_after_actions = 4\n",
    "# update_target_network = 10000\n",
    "\n",
    "# # Using huber loss for stability (specifically for Adam)\n",
    "# loss_function = keras.losses.Huber()\n",
    "\n",
    "# # Initialize history variables\n",
    "# action_history = []\n",
    "# state_history = []\n",
    "# state_next_history = []\n",
    "# rewards_history = []\n",
    "# done_history = []\n",
    "# episode_reward_history = []\n",
    "# running_reward = 0\n",
    "# episode_count = 0\n",
    "# frame_count = 0\n",
    "\n",
    "# def create_q_model():\n",
    "#     return keras.Sequential(\n",
    "#         [\n",
    "#             layers.Input(shape=(84, 84, 4)),\n",
    "#             layers.Conv2D(32, kernel_size=8, strides=4, activation=\"relu\"),\n",
    "#             layers.Conv2D(64, kernel_size=4, strides=2, activation=\"relu\"),\n",
    "#             layers.Conv2D(64, kernel_size=3, strides=1, activation=\"relu\"),\n",
    "#             layers.Flatten(),\n",
    "#             layers.Dense(512, activation=\"relu\"),\n",
    "#             layers.Dense(num_actions, activation=\"linear\")\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "# model = create_q_model()\n",
    "# model_target = create_q_model()\n",
    "\n",
    "# optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# # Create directory for saving models\n",
    "# save_dir = \"models\"\n",
    "# os.makedirs(save_dir, exist_ok=True)  # Replay buffer\n",
    "# replay_buffer = deque(maxlen=max_memory_length)\n",
    "\n",
    "# # Initialize variables\n",
    "# best_running_reward = -float('inf')  # Start with the lowest possible reward\n",
    "# last_saved_model = None  # Track the last saved model\n",
    "# log_file = \"model_performance_log.txt\"  # Log file path\n",
    "\n",
    "# # Clear log file\n",
    "# with open(log_file, \"w\") as f:\n",
    "#     f.write(\"Episode\\tRunning Reward\\tEpsilon\\n\")\n",
    "\n",
    "# # Initialize history\n",
    "# episode_reward_history = deque(maxlen=100)  # Efficient rolling window\n",
    "\n",
    "# frame_count = 0\n",
    "# episode_count = 0\n",
    "# epsilon = epsilon_max\n",
    "\n",
    "# # Target network update tau for soft updates\n",
    "# tau = 0.001\n",
    "\n",
    "# while True:\n",
    "#     # Reset environment\n",
    "#     observation, _ = env.reset()\n",
    "#     state = tf.expand_dims(tf.convert_to_tensor(observation, dtype=tf.float32), 0)  # Preprocessed state\n",
    "#     episode_reward = 0\n",
    "\n",
    "#     # Epsilon decay once per episode\n",
    "#     epsilon = max(epsilon_min, epsilon_max - epsilon_interval * (frame_count / epsilon_greedy_frames))\n",
    "\n",
    "#     for timestep in range(1, max_steps_per_episode):\n",
    "#         frame_count += 1\n",
    "\n",
    "#         # Epsilon-greedy action selection\n",
    "#         if frame_count < epsilon_random_frames or tf.random.uniform((1,)) < epsilon:\n",
    "#             action = env.action_space.sample()\n",
    "#         else:\n",
    "#             # Ensure state is in the correct format: (batch_size, height, width, channels)\n",
    "#             state = tf.transpose(state, perm=[0, 2, 3, 1])  # Convert (1, 4, 84, 84) to (1, 84, 84, 4) if needed\n",
    "#             action_probs = model(state, training=False)\n",
    "#             action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "#         # Step in the environment\n",
    "#         state_next, reward, done, _, _ = env.step(action)\n",
    "#         state_next = tf.expand_dims(tf.convert_to_tensor(state_next, dtype=tf.float32), 0)  # Preprocessed next state\n",
    "#         episode_reward += reward\n",
    "\n",
    "#         # Store transition in replay buffer\n",
    "#         replay_buffer.append((state, action, reward, state_next, done))\n",
    "#         state = state_next  # Update state\n",
    "\n",
    "#         # Train the model\n",
    "#         if frame_count % update_after_actions == 0 and len(replay_buffer) >= batch_size:\n",
    "#             # Sample a batch from replay buffer\n",
    "#             batch = random.sample(replay_buffer, batch_size)\n",
    "#             states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "#             # Ensure all tensors are in correct shape\n",
    "#             state_sample = [tf.transpose(s, perm=[0, 2, 3, 1]) for s in states]\n",
    "#             state_sample = tf.concat(state_sample, axis=0)\n",
    "#             state_sample = tf.cast(state_sample, dtype=tf.float32)\n",
    "\n",
    "#             next_state_sample = [tf.transpose(s, perm=[0, 2, 3, 1]) for s in next_states]\n",
    "#             next_state_sample = tf.concat(next_state_sample, axis=0)\n",
    "#             next_state_sample = tf.cast(next_state_sample, dtype=tf.float32)\n",
    "\n",
    "#             action_sample = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "#             reward_sample = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "#             done_sample = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "#             # Compute target Q-values\n",
    "#             future_rewards = model_target(next_state_sample)\n",
    "#             updated_q_values = reward_sample + gamma * tf.reduce_max(future_rewards, axis=1) * (1 - done_sample)\n",
    "\n",
    "#             # Masked loss\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 q_values = model(state_sample)\n",
    "#                 q_action = tf.reduce_sum(tf.one_hot(action_sample, num_actions) * q_values, axis=1)\n",
    "#                 loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "#             # Backpropagation\n",
    "#             grads = tape.gradient(loss, model.trainable_variables)\n",
    "#             optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "#         # Update target network\n",
    "#         if frame_count % update_target_network == 0:\n",
    "#             model_target.set_weights([\n",
    "#                 tau * w + (1 - tau) * tw\n",
    "#                 for w, tw in zip(model.get_weights(), model_target.get_weights())\n",
    "#             ])\n",
    "\n",
    "#         # End episode if done\n",
    "#         if done:\n",
    "#             break\n",
    "\n",
    "#     # Update running reward and history\n",
    "#     episode_reward_history.append(episode_reward)\n",
    "#     running_reward = np.mean(episode_reward_history)\n",
    "#     episode_count += 1\n",
    "\n",
    "#     # Check if the model should be saved\n",
    "#     if running_reward > best_running_reward:\n",
    "#         best_running_reward = running_reward\n",
    "\n",
    "#         # Remove the last saved model if it exists\n",
    "#         if last_saved_model and os.path.exists(last_saved_model):\n",
    "#             os.remove(last_saved_model)\n",
    "\n",
    "#         # Save the new best model\n",
    "#         model_path = f\"models/best_model_episode_{episode_count}.keras\"\n",
    "#         model.save(model_path)\n",
    "#         last_saved_model = model_path\n",
    "\n",
    "#         # Log the result\n",
    "#         with open(log_file, \"a\") as f:\n",
    "#             f.write(f\"{episode_count}\\t{running_reward:.2f}\\t{epsilon:.2f}\\n\")\n",
    "\n",
    "#         print(f\"New best model saved: {model_path} with running reward: {running_reward:.2f}\")\n",
    "\n",
    "#     # Print progress\n",
    "#     if episode_count % 10 == 0:\n",
    "#         print(f\"Episode {episode_count}, Frame {frame_count}, Running Reward: {running_reward:.2f}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "#     # Termination conditions\n",
    "#     if running_reward > 10000:\n",
    "#         print(f\"Solved with a running_reward of {running_reward} at episode {episode_count}!\")\n",
    "#         model.save(\"spaceinvaders_qmodel_solved.keras\")\n",
    "#         break\n",
    "\n",
    "#     if max_episodes > 0 and episode_count >= max_episodes:\n",
    "#         print(f\"Stopped at episode {episode_count}!\")\n",
    "#         model.save(f\"spaceinvaders_qmodel_final_episode_{episode_count}.keras\")\n",
    "#         break\n",
    "\n",
    "#     if max_frames > 0 and frame_count >= max_frames:\n",
    "#         print(f\"Stopped at frame {frame_count}!\")\n",
    "#         model.save(f\"spaceinvaders_qmodel_final_frame_{frame_count}.keras\")\n",
    "#         break\n",
    "\n",
    "# # Final save after training\n",
    "# model.save(\"spaceinvaders_qmodel_final.keras\")\n",
    "# print(\"Final model saved as 'spaceinvaders_qmodel_final.keras'.\")\n",
    "# print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstate_sample\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'state_sample' is not defined"
     ]
    }
   ],
   "source": [
    "print(state_sample.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
