{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou don’t need import ale_py and gym.register_envs(ale_py) because:\\n\\nAtari environments are automatically registered during the installation of gymnasium[atari].\\nThe current Gymnasium API handles all the behind-the-scenes setup for you.\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FrameStackObservation, AtariPreprocessing, RecordVideo\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "'''\n",
    "You don’t need import ale_py and gym.register_envs(ale_py) because:\n",
    "\n",
    "Atari environments are automatically registered during the installation of gymnasium[atari].\n",
    "The current Gymnasium API handles all the behind-the-scenes setup for you.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "seed = 42\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 1.0\n",
    "epsilon_interval = (epsilon_max - epsilon_min)\n",
    "epsilon_decay_frames = 1000000  # Adjust this value if needed\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "max_steps_per_episode = 10000\n",
    "max_episodes = 0 # Update this\n",
    "max_frames = 1e7\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon_random_frames = 50000\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "\n",
    "# Replay buffer parameters\n",
    "# NOTE: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000\n",
    "update_after_actions = 4\n",
    "update_target_network = 10000\n",
    "\n",
    "# Using huber loss for stability (specifically for Adam)\n",
    "loss_function = keras.losses.Huber()\n",
    "\n",
    "# Initialize history variables\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\glajusj\\Documents\\github\\Deep-Learning-Julia\\.env\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:416: UserWarning: \u001b[33mWARN: Unable to save last video! Did you call close()?\u001b[0m\n",
      "  logger.warn(\"Unable to save last video! Did you call close()?\")\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "# Setting frameskip=1 disables frame-skipping in the base environment, ensuring no conflict with AtariPreprocessing.\n",
    "#The Atari environment (ALE/SpaceInvaders-v5) has a default frameskip value (usually [2, 5]).\n",
    "env = gym.make(\"SpaceInvadersNoFrameskip-v4\", render_mode=\"rgb_array\", frameskip=1) \n",
    "env = AtariPreprocessing(env)\n",
    "env = FrameStackObservation(env, 4)\n",
    "trigger = lambda t: t % 20 == 0 # Every 20th episode\n",
    "env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 6\n",
      "Actions and their meanings: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# Get the number of actions\n",
    "num_actions = env.action_space.n\n",
    "print(f\"Number of actions: {num_actions}\")\n",
    "# List all action meanings\n",
    "action_meanings = env.unwrapped.get_action_meanings()\n",
    "print(\"Actions and their meanings:\", action_meanings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_model():\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(84, 84, 4)),\n",
    "            layers.Conv2D(32, kernel_size=8, strides=4, activation=\"relu\"),\n",
    "            layers.Conv2D(64, kernel_size=4, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(64, kernel_size=3, strides=1, activation=\"relu\"),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(512, activation=\"relu\"),\n",
    "            layers.Dense(num_actions, activation=\"linear\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "model = create_q_model()\n",
    "model_target = create_q_model()\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Create directory for saving models\n",
    "save_dir = \"models\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "state_sample shape: (32, 84, 84, 4)\n",
      "next_state_sample shape: (32, 84, 84, 4)\n",
      "Model saved for episode 1 at models/episode_1.keras.\n",
      "Solved with a running_reward of 220.0 at episode 1!\n",
      "Final model saved as 'breakout_qmodel_final.keras'.\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Replay buffer\n",
    "replay_buffer = deque(maxlen=max_memory_length)\n",
    "\n",
    "# Initialize history\n",
    "episode_reward_history = deque(maxlen=100)  # Efficient rolling window\n",
    "\n",
    "frame_count = 0\n",
    "episode_count = 0\n",
    "epsilon = epsilon_max\n",
    "\n",
    "# Target network update tau for soft updates\n",
    "tau = 0.001\n",
    "\n",
    "while True:\n",
    "    # Reset environment\n",
    "    observation, _ = env.reset()\n",
    "    state = tf.expand_dims(tf.convert_to_tensor(observation, dtype=tf.float32), 0)  # Preprocessed state\n",
    "    episode_reward = 0\n",
    "\n",
    "    # Epsilon decay once per episode\n",
    "    epsilon = max(epsilon_min, epsilon_max - epsilon_interval * (frame_count / epsilon_greedy_frames))\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if frame_count < epsilon_random_frames or tf.random.uniform((1,)) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action_probs = model(state, training=False)\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Step in the environment\n",
    "        state_next, reward, done, _, _ = env.step(action)\n",
    "        state_next = tf.expand_dims(tf.convert_to_tensor(state_next, dtype=tf.float32), 0)  # Preprocessed next state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Store transition in replay buffer\n",
    "        replay_buffer.append((state, action, reward, state_next, done))\n",
    "        state = state_next  # Update state\n",
    "\n",
    "        # Train the model\n",
    "        if frame_count % update_after_actions == 0 and len(replay_buffer) >= batch_size:\n",
    "            # Sample a batch from replay buffer\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            # Convert to tensors\n",
    "            state_sample = tf.concat(states, axis=0)  # Combine into a single tensor\n",
    "            state_sample = tf.transpose(state_sample, perm=[0, 2, 3, 1])  # Rearrange to (batch_size, 84, 84, 4)\n",
    "            state_sample = tf.cast(state_sample, dtype=tf.float32)\n",
    "            \n",
    "            next_state_sample = tf.concat(next_states, axis=0)  # Combine into a single tensor\n",
    "            next_state_sample = tf.transpose(next_state_sample, perm=[0, 2, 3, 1])  # Rearrange to (batch_size, 84, 84, 4)\n",
    "            next_state_sample = tf.cast(next_state_sample, dtype=tf.float32)\n",
    "\n",
    "            action_sample = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "            reward_sample = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "            done_sample = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "            # Debugging shape before passing to the model\n",
    "           # print(\"state_sample shape:\", state_sample.shape)  # Should be (batch_size, 84, 84, 4)\n",
    "           # print(\"next_state_sample shape:\", next_state_sample.shape)  # Should be (batch_size, 84, 84, 4)\n",
    "\n",
    "\n",
    "            # Compute target Q-values\n",
    "            future_rewards = model_target(next_state_sample)\n",
    "            updated_q_values = reward_sample + gamma * tf.reduce_max(future_rewards, axis=1) * (1 - done_sample)\n",
    "\n",
    "            # Masked loss\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = model(state_sample)\n",
    "                q_action = tf.reduce_sum(tf.one_hot(action_sample, num_actions) * q_values, axis=1)\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Update target network\n",
    "        if frame_count % update_target_network == 0:\n",
    "            model_target.set_weights([\n",
    "                tau * w + (1 - tau) * tw\n",
    "                for w, tw in zip(model.get_weights(), model_target.get_weights())\n",
    "            ])\n",
    "\n",
    "        # End episode if done\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update running reward and history\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    episode_count += 1\n",
    "\n",
    "    # Save model after each episode\n",
    "    model_path = f\"models/episode_{episode_count}.keras\"\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved for episode {episode_count} at {model_path}.\")\n",
    "\n",
    "    # Print progress\n",
    "    if episode_count % 10 == 0:\n",
    "        print(f\"Episode {episode_count}, Frame {frame_count}, Running Reward: {running_reward:.2f}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "    # Termination conditions\n",
    "    if running_reward > 40:  # Considered good intermediate performance\n",
    "        print(f\"Solved with a running_reward of {running_reward} at episode {episode_count}!\")\n",
    "        model.save(\"spaceinvaders_qmodel_solved.keras\")\n",
    "        break\n",
    "\n",
    "    if max_episodes > 0 and episode_count >= max_episodes:\n",
    "        print(f\"Stopped at episode {episode_count}!\")\n",
    "        break\n",
    "\n",
    "    if max_frames > 0 and frame_count >= max_frames:\n",
    "        print(f\"Stopped at frame {frame_count}!\")\n",
    "        break\n",
    "\n",
    "# Final save after training\n",
    "model.save(\"spaceinvaders_qmodel_final.keras\")\n",
    "print(\"Final model saved as 'breakout_qmodel_final.keras'.\")\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from gymnasium.wrappers import FrameStackObservation\n",
    "from gymnasium.wrappers.atari_preprocessing import AtariPreprocessing\n",
    "\n",
    "# Register ALE environments\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_file = \"../Lab/breakout_qmodel_final.keras\"\n",
    "agent = keras.models.load_model(model_file)\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make(\"SpaceInvadersNoFrameskip-v4\", render_mode=\"human\")\n",
    "env = AtariPreprocessing(env)\n",
    "env = FrameStackObservation(env, 4)\n",
    "\n",
    "# Reset the environment\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "# Run the environment loop\n",
    "while not done:\n",
    "    # Convert state to a tensor for compute efficiency\n",
    "    state_tensor = keras.ops.convert_to_tensor(state)\n",
    "    # Transpose state shape from (4, 84, 84) to (84, 84, 4)\n",
    "    state_tensor = keras.ops.transpose(state_tensor, [1, 2, 0])\n",
    "    # Add batch dimension\n",
    "    state_tensor = keras.ops.expand_dims(state_tensor, 0)\n",
    "    # Predict action probabilities\n",
    "    action_probs = agent(state_tensor, training=False)\n",
    "    # Take the \"best\" action\n",
    "    action = keras.ops.argmax(action_probs[0]).numpy()\n",
    "\n",
    "    # Step the environment\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
