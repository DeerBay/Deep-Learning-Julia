{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeerBay/Deep-Learning-Julia/blob/main/sequential_reward_shaping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Space Invaders with Deep Q-Learning and Reward Shaping\n",
        "This notebook demonstrates a reinforcement learning project to train an agent to play Space Invaders using the Deep Q-Learning (DQN) algorithm. To enhance learning efficiency, reward shaping techniques are applied to guide the agent's behavior toward desired outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "The first few cells handle the installation of required libraries and the setup of Google Drive for saving models and logs. Ensure all dependencies are installed and paths are correctly set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reward Shaping\n",
        "The `RewardShaper` class adds domain knowledge to the reward signal. For instance, it:\n",
        "- Encourages score improvements with small bonus rewards.\n",
        "- Penalizes losing lives or inactivity.\n",
        "- Incentivizes movement and shooting with balanced adjustments.\n",
        "\n",
        "These modifications help guide the agent toward optimal behavior.\n",
        "\n",
        "I've tried three different reward shapings here to examine wether it is possible to train the model for shorter time with effecient result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep Q-Learning Agent\n",
        "The DQN agent uses a convolutional neural network to approximate the Q-value function. Key components include:\n",
        "- **Network Architecture**: Three convolutional layers followed by a fully connected layer.\n",
        "- **Action Selection**: Epsilon-greedy policy balances exploration and exploitation.\n",
        "- **Target Network**: A separate target network helps stabilize Q-value updates.\n",
        "- **Training**: The agent minimizes the Huber loss between predicted and target Q-values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Process\n",
        "The main training loop involves resetting the environment, interacting with it using the agent's policy, and updating the Q-network using experience replay. Metrics such as shaped rewards, original rewards, and epsilon values are logged for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization\n",
        "After each try I've generated a plot showing the training progress:\n",
        "- **Average Reward**: Monitors the agent's performance over episodes.\n",
        "- **Epsilon**: Tracks the exploration rate as it decays over time.\n",
        "\n",
        "These insights help evaluate the effectiveness of the training and reward shaping techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_oJLHLqkPfSt",
        "outputId": "125dac77-f175-4f39-a085-80d54d698cc5"
      },
      "outputs": [],
      "source": [
        "!pip install 'gymnasium[atari, other]' gymnasium tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtRyIA9oclhD",
        "outputId": "88992875-f193-4a45-c9eb-161b0a306c2b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Try 1\n",
        "\n",
        "Looking at the reward shaping implementation in the code, the key aspects of preferred rewards are:\n",
        "\n",
        "Score-Based Rewards:\n",
        "\n",
        "\n",
        "Improvements in game score are rewarded, but with a moderate multiplier (0.05)\n",
        "This is intentionally reduced from 0.1 to avoid over-emphasizing score chasing\n",
        "\n",
        "\n",
        "Survival-Based Rewards:\n",
        "\n",
        "\n",
        "Loss of lives is penalized (-0.5)\n",
        "This creates a strong incentive for survival behavior\n",
        "\n",
        "\n",
        "Movement-Based Rewards:\n",
        "\n",
        "\n",
        "Small reward (0.005) for movement to encourage exploration\n",
        "Penalties (-0.05) for staying still too long (after 15 frames)\n",
        "This balance prevents both camping and excessive movement\n",
        "\n",
        "\n",
        "Shooting Behavior:\n",
        "\n",
        "\n",
        "Moderate reward (0.02) for shooting actions\n",
        "Small penalty (-0.02) for not shooting for extended periods (>30 frames)\n",
        "The timing and values are carefully balanced to encourage periodic but not constant shooting\n",
        "\n",
        "\n",
        "Reward Bounds:\n",
        "\n",
        "\n",
        "A floor of -0.5 is implemented to prevent extreme negative rewards\n",
        "This helps maintain stable learning by avoiding reward collapse\n",
        "\n",
        "The overall philosophy appears to prefer balanced, moderate rewards that:\n",
        "\n",
        "Don't overemphasize any single behavior\n",
        "Encourage a mix of survival, scoring, and tactical play\n",
        "Avoid extreme values that could destabilize learning\n",
        "Create natural-feeling gameplay without exploitable patterns\n",
        "\n",
        "Would you like me to elaborate on any of these aspects or explain why certain reward values were chosen over others?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "n2Rgm9ToUuN4",
        "outputId": "2278bce8-f98e-42e4-b02d-108592b7759b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import FrameStackObservation, AtariPreprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "import ale_py\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size, state_shape, num_actions):\n",
        "        self.max_size = max_size\n",
        "        self.states = np.zeros((max_size, *state_shape), dtype=np.float32)\n",
        "        self.actions = np.zeros(max_size, dtype=np.int32)\n",
        "        self.rewards = np.zeros(max_size, dtype=np.float32)\n",
        "        self.next_states = np.zeros((max_size, *state_shape), dtype=np.float32)\n",
        "        self.dones = np.zeros(max_size, dtype=np.float32)\n",
        "        self.index = 0\n",
        "        self.is_full = False\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        idx = self.index % self.max_size\n",
        "        self.states[idx] = state\n",
        "        self.actions[idx] = action\n",
        "        self.rewards[idx] = reward\n",
        "        self.next_states[idx] = next_state\n",
        "        self.dones[idx] = done\n",
        "        self.index += 1\n",
        "        if self.index >= self.max_size:\n",
        "            self.is_full = True\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        max_index = self.max_size if self.is_full else self.index\n",
        "        indices = np.random.choice(max_index, batch_size, replace=False)\n",
        "        return (\n",
        "            self.states[indices],\n",
        "            self.actions[indices],\n",
        "            self.rewards[indices],\n",
        "            self.next_states[indices],\n",
        "            self.dones[indices]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.max_size if self.is_full else self.index\n",
        "\n",
        "class RewardShaper:\n",
        "    def __init__(self):\n",
        "        self.previous_score = 0\n",
        "        self.previous_lives = 3\n",
        "        self.steps_since_last_shot = 0\n",
        "        self.last_known_position = None\n",
        "        self.consecutive_no_movement = 0\n",
        "\n",
        "    def calculate_shaped_reward(self, info, reward, action, position):\n",
        "        shaped_reward = reward  # Start with original reward\n",
        "\n",
        "        # Track score improvements - reduced multiplier\n",
        "        current_score = info.get('score', 0)\n",
        "        if current_score > self.previous_score:\n",
        "            score_improvement = current_score - self.previous_score\n",
        "            shaped_reward += 0.05 * score_improvement  # Reduced from 0.1 to 0.05\n",
        "            self.previous_score = current_score\n",
        "\n",
        "        # Penalize death\n",
        "        current_lives = info.get('lives', 3)\n",
        "        if current_lives < self.previous_lives:\n",
        "            shaped_reward -= 0.5\n",
        "            self.previous_lives = current_lives\n",
        "\n",
        "        # Movement-based shaping\n",
        "        if self.last_known_position is not None:\n",
        "            movement = abs(position - self.last_known_position)\n",
        "            if movement < 0.005:\n",
        "                self.consecutive_no_movement += 1\n",
        "                if self.consecutive_no_movement > 15:\n",
        "                    shaped_reward -= 0.05\n",
        "            else:\n",
        "                self.consecutive_no_movement = 0\n",
        "                # Smaller movement reward\n",
        "                shaped_reward += 0.005\n",
        "\n",
        "        self.last_known_position = position\n",
        "\n",
        "        # Shooting behavior shaping - more balanced\n",
        "        if action == 1:  # Shoot action\n",
        "            self.steps_since_last_shot = 0\n",
        "            shaped_reward += 0.02  # Reduced from 0.05 to 0.02\n",
        "        else:\n",
        "            self.steps_since_last_shot += 1\n",
        "            if self.steps_since_last_shot > 30:  # Increased from 20 to 30\n",
        "                shaped_reward -= 0.02  # Reduced from 0.05 to 0.02\n",
        "\n",
        "        # Ensure reward doesn't go too negative\n",
        "        return max(-0.5, shaped_reward)  # Add a floor to prevent extreme negative rewards\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_shape, num_actions, learning_rate=0.00025):\n",
        "        self.state_shape = state_shape\n",
        "        self.num_actions = num_actions\n",
        "        self.model = self.create_q_model(state_shape, num_actions)\n",
        "        self.target_model = self.create_q_model(state_shape, num_actions)\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
        "        self.replay_buffer = ReplayBuffer(max_size=100000, state_shape=state_shape, num_actions=num_actions)\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.1\n",
        "        self.epsilon_max = 1.0\n",
        "        self.epsilon_random_frames = 10000  # Initial random frames\n",
        "        self.epsilon_greedy_frames = 100000.0  # Total frames over which to decay epsilon\n",
        "\n",
        "    def create_q_model(self, input_shape, num_actions):\n",
        "        return tf.keras.Sequential([\n",
        "            layers.Input(shape=input_shape),\n",
        "            layers.Conv2D(32, kernel_size=8, strides=4, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "            layers.Conv2D(64, kernel_size=4, strides=2, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "            layers.Conv2D(64, kernel_size=3, strides=1, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(512, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "            layers.Dense(num_actions, activation=\"linear\")\n",
        "        ])\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.randint(self.num_actions)\n",
        "        state_tensor = tf.convert_to_tensor(state[np.newaxis, ...], dtype=tf.float32)\n",
        "        q_values = self.model(state_tensor, training=False)\n",
        "        return tf.argmax(q_values[0]).numpy()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, states, actions, rewards, next_states, dones):\n",
        "        future_rewards = self.target_model(next_states, training=False)\n",
        "        target_q_values = rewards + self.gamma * tf.reduce_max(future_rewards, axis=1) * (1 - dones)\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = self.model(states, training=True)\n",
        "            q_action = tf.reduce_sum(tf.one_hot(actions, self.num_actions) * q_values, axis=1)\n",
        "            loss = tf.keras.losses.Huber()(target_q_values, q_action)\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "def get_agent_position(state):\n",
        "    \"\"\"Extract the agent's position from the state\"\"\"\n",
        "    bottom_slice = state[-10:, :, -1]  # Look at the bottom portion of the last frame\n",
        "    agent_x = np.mean(np.where(bottom_slice > 0.5)[1]) if np.any(bottom_slice > 0.5) else None\n",
        "    if agent_x is not None:\n",
        "        return agent_x / state.shape[1]  # Normalize position\n",
        "    return 0.5  # Return center if agent not found\n",
        "\n",
        "def main():\n",
        "    # Paths for saving\n",
        "    model_dir = \"/content/drive/MyDrive/SpaceInvaders/models\"\n",
        "    plot_dir = \"/content/drive/MyDrive/SpaceInvaders/plots\"\n",
        "    log_dir = \"/content/drive/MyDrive/SpaceInvaders/logs\"\n",
        "    log_file_path = os.path.join(log_dir, \"training_log.txt\")\n",
        "\n",
        "    # Create all necessary directories\n",
        "    for directory in [model_dir, plot_dir, log_dir]:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Environment setup\n",
        "    gym.register_envs(ale_py)\n",
        "    env = gym.make(\"SpaceInvadersNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
        "    env = AtariPreprocessing(env, frame_skip=4, grayscale_obs=True, scale_obs=True)\n",
        "    env = FrameStackObservation(env, stack_size=4)\n",
        "\n",
        "    # Agent and reward shaper setup\n",
        "    state_shape = (84, 84, 4)\n",
        "    num_actions = env.action_space.n\n",
        "    agent = DQNAgent(state_shape, num_actions)\n",
        "    reward_shaper = RewardShaper()\n",
        "\n",
        "    # Training parameters\n",
        "    max_episodes = 750  # Reduced from 2000\n",
        "    max_steps_per_episode = 3000\n",
        "    update_target_every = 1000\n",
        "    log_every = 1\n",
        "    episode_rewards = []\n",
        "    frame_count = 0\n",
        "    best_avg_reward = -float('inf')\n",
        "\n",
        "    with open(log_file_path, \"w\") as f:\n",
        "        f.write(\"Episode,Avg_Reward,Epsilon,Original_Reward,Shaped_Reward\\n\")\n",
        "\n",
        "    # Training loop\n",
        "    for episode in range(max_episodes):\n",
        "        state, info = env.reset()\n",
        "        state = np.array(state, dtype=np.float32) / 255.0\n",
        "        state = np.transpose(state, (1, 2, 0))\n",
        "        episode_reward = 0\n",
        "        episode_original_reward = 0\n",
        "        episode_steps = 0\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            frame_count += 1\n",
        "            episode_steps += 1\n",
        "\n",
        "            # Choose action\n",
        "            if frame_count < agent.epsilon_random_frames:\n",
        "                action = np.random.randint(num_actions)\n",
        "            else:\n",
        "                action = agent.get_action(state)\n",
        "\n",
        "            # Step environment\n",
        "            next_state, reward, done, _, info = env.step(action)\n",
        "            next_state = np.array(next_state, dtype=np.float32) / 255.0\n",
        "            next_state = np.transpose(next_state, (1, 2, 0))\n",
        "\n",
        "            # Shape the reward\n",
        "            agent_position = get_agent_position(state)\n",
        "            shaped_reward = reward_shaper.calculate_shaped_reward(info, reward, action, agent_position)\n",
        "\n",
        "            # Store original reward for logging\n",
        "            episode_original_reward += reward\n",
        "            episode_reward += shaped_reward\n",
        "\n",
        "            # Store in replay buffer\n",
        "            agent.replay_buffer.add(state, action, shaped_reward, next_state, done)\n",
        "            state = next_state\n",
        "\n",
        "            # Train\n",
        "            if len(agent.replay_buffer) >= 32:\n",
        "                states, actions, rewards, next_states, dones = agent.replay_buffer.sample(32)\n",
        "                agent.train_step(states, actions, rewards, next_states, dones)\n",
        "\n",
        "            # Update target model\n",
        "            if frame_count % update_target_every == 0:\n",
        "                agent.update_target_model()\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Episode-based epsilon decay\n",
        "        if frame_count >= agent.epsilon_random_frames:\n",
        "            decay_per_episode = (agent.epsilon_max - agent.epsilon_min) / (max_episodes * 0.3)  # Decay over first 30% of episodes\n",
        "            agent.epsilon = max(agent.epsilon_min, agent.epsilon - decay_per_episode)\n",
        "\n",
        "        episode_rewards.append(episode_reward)\n",
        "        avg_reward = np.mean(episode_rewards[-log_every:])\n",
        "\n",
        "        # Log both original and shaped rewards\n",
        "        with open(log_file_path, \"a\") as f:\n",
        "            f.write(f\"{episode + 1},{avg_reward:.2f},{agent.epsilon:.4f},{episode_original_reward:.2f},{episode_reward:.2f}\\n\")\n",
        "\n",
        "        print(f\"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}, Original Reward: {episode_original_reward:.2f}, \"\n",
        "              f\"Shaped Reward: {episode_reward:.2f}, Epsilon: {agent.epsilon:.4f}, Frame Count: {frame_count}, \"\n",
        "              f\"Steps: {episode_steps}\")\n",
        "\n",
        "        # Save model conditionally\n",
        "        if (episode + 1) % 100 == 0 or avg_reward > best_avg_reward:\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "                print(f\"New best average reward: {best_avg_reward:.2f}. Saving best model.\")\n",
        "            agent.model.save(f\"{model_dir}/episode{episode + 1}_space_invaders_model.keras\")\n",
        "\n",
        "    # Save final model\n",
        "    agent.model.save(f\"{model_dir}/final_space_invaders_model.keras\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "id": "4-Uq6iNk7F6n",
        "outputId": "a1c03449-09ca-41f3-f979-de562e1a2317"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "\n",
        "filepath ='/content/drive/MyDrive/SpaceInvaders/logs/training_log.txt'\n",
        "\n",
        "df = pd.read_csv(filepath, sep=',')\n",
        "\n",
        "# Directory to save the plot\n",
        "save_path = '/content/drive/MyDrive/SpaceInvaders/plots/training_log_plot_1.png'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with two y-axes\n",
        "fig, ax1 = plt.subplots(figsize=(15, 7))\n",
        "\n",
        "# Plot Average Reward on the primary y-axis\n",
        "color1 = 'blue'\n",
        "ax1.set_xlabel('Episode')\n",
        "ax1.set_ylabel('Average Reward', color=color1)\n",
        "ax1.plot(df['Episode'], df['Avg_Reward'], color=color1, alpha=0.7, label='Average Reward')\n",
        "ax1.tick_params(axis='y', labelcolor=color1)\n",
        "\n",
        "# Create a secondary y-axis for Epsilon\n",
        "ax2 = ax1.twinx()\n",
        "color2 = 'red'\n",
        "ax2.set_ylabel('Epsilon', color=color2)\n",
        "ax2.plot(df['Episode'], df['Epsilon'], color=color2, alpha=0.7, label='Epsilon')\n",
        "ax2.tick_params(axis='y', labelcolor=color2)\n",
        "\n",
        "# Title and layout\n",
        "plt.title('Reinforcement Learning Training Log: Reward and Exploration Rate', fontsize=16)\n",
        "fig.tight_layout()\n",
        "\n",
        "# Add a legend\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "# Grid for better readability\n",
        "ax1.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Save the plot\n",
        "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"Plot saved at: {save_path}\")\n",
        "\n",
        "# Optional: Calculate and print some statistics\n",
        "print(f\"Total Episodes: {len(df)}\")\n",
        "print(f\"\\nReward Statistics:\")\n",
        "print(f\"Minimum Reward: {df['Avg_Reward'].min():.2f}\")\n",
        "print(f\"Maximum Reward: {df['Avg_Reward'].max():.2f}\")\n",
        "print(f\"Mean Reward: {df['Avg_Reward'].mean():.2f}\")\n",
        "print(f\"Median Reward: {df['Avg_Reward'].median():.2f}\")\n",
        "\n",
        "print(f\"\\nEpsilon Statistics:\")\n",
        "print(f\"Initial Epsilon: {df['Epsilon'].iloc[0]:.4f}\")\n",
        "print(f\"Final Epsilon: {df['Epsilon'].iloc[-1]:.4f}\")\n",
        "\n",
        "# Show the plot (optional, depends on your environment)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outcome Try 1\n",
        "\n",
        "1. Epsilon Decay (Red Line):\n",
        "- Starts at 1.0 (100% random actions)\n",
        "- Steadily decreases until around episode 300\n",
        "- Stabilizes at the minimum value (0.1) for the remainder of training\n",
        "- This indicates a proper transition from exploration to exploitation\n",
        "\n",
        "2. Average Reward Performance (Blue Line):\n",
        "- Initial performance is relatively low and unstable (0-200 episodes)\n",
        "- Shows increasing variance and higher peaks as training progresses\n",
        "- Notable spikes reaching ~600-1000 rewards occur periodically\n",
        "- The baseline performance seems to stabilize around 200-400 rewards\n",
        "\n",
        "3. Learning Characteristics:\n",
        "- High variance in performance even after epsilon stabilization\n",
        "- No clear consistent upward trend in average reward\n",
        "- Periodic high-performance episodes suggest the agent learns useful strategies but struggles with consistency\n",
        "\n",
        "4. Potential Issues:\n",
        "- The high variance in rewards might indicate:\n",
        "  - Sensitivity to initial conditions in episodes\n",
        "  - Possible instability in the reward shaping mechanism\n",
        "  - Challenge in maintaining consistent performance\n",
        "\n",
        "This outcome suggests that while the agent can achieve high scores occasionally, the reward shaping approach might need refinement to:\n",
        "- Reduce performance variance\n",
        "- Improve consistency in learned behaviors\n",
        "- Better stabilize the baseline performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Try 2\n",
        "\n",
        "Looking at the code for this modified version, there are several changes that align with addressing the high variance issues we observed:\n",
        "\n",
        "1. Improved Epsilon Decay Strategy:\n",
        "- Changed from 30% to 70% of episodes for decay period (`max_episodes * 0.7`)\n",
        "- Lowered epsilon_min from 0.1 to 0.01\n",
        "- Increased initial random frames from 10,000 to 50,000\n",
        "- This should provide more stable exploration and better policy learning\n",
        "\n",
        "2. More Balanced Reward Shaping:\n",
        "- Death penalty reduced but still significant (-0.5)\n",
        "- Movement thresholds made more lenient (0.005 vs 0.01)\n",
        "- Shooting behavior rewards balanced (0.02) with longer tolerance (30 frames vs 20)\n",
        "- Floor added to prevent extreme negative rewards (max(-0.5, shaped_reward))\n",
        "\n",
        "3. Training Parameters:\n",
        "- Increased max_episodes from 750 to 1000\n",
        "- Kept max_steps_per_episode at 3000\n",
        "- Maintained update_target_every at 1000\n",
        "\n",
        "These changes should help by:\n",
        "- Allowing more time for exploration and policy refinement\n",
        "- Reducing reward volatility while maintaining meaningful signals\n",
        "- Creating a smoother learning curve with less extreme variations\n",
        "\n",
        "I believe this could help address the high variance issues we saw in the previous training results. Would you like me to explain any specific aspect of these modifications in more detail?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr1-LHB_suk2",
        "outputId": "12a78749-4acb-45fa-de4c-ea8ad8a9a7de"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import FrameStackObservation, AtariPreprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "import ale_py\n",
        "import matplotlib as plt\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size, state_shape, num_actions):\n",
        "        self.max_size = max_size\n",
        "        self.states = np.zeros((max_size, *state_shape), dtype=np.float32)\n",
        "        self.actions = np.zeros(max_size, dtype=np.int32)\n",
        "        self.rewards = np.zeros(max_size, dtype=np.float32)\n",
        "        self.next_states = np.zeros((max_size, *state_shape), dtype=np.float32)\n",
        "        self.dones = np.zeros(max_size, dtype=np.float32)\n",
        "        self.index = 0\n",
        "        self.is_full = False\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        idx = self.index % self.max_size\n",
        "        self.states[idx] = state\n",
        "        self.actions[idx] = action\n",
        "        self.rewards[idx] = reward\n",
        "        self.next_states[idx] = next_state\n",
        "        self.dones[idx] = done\n",
        "        self.index += 1\n",
        "        if self.index >= self.max_size:\n",
        "            self.is_full = True\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        max_index = self.max_size if self.is_full else self.index\n",
        "        indices = np.random.choice(max_index, batch_size, replace=False)\n",
        "        return (\n",
        "            self.states[indices],\n",
        "            self.actions[indices],\n",
        "            self.rewards[indices],\n",
        "            self.next_states[indices],\n",
        "            self.dones[indices]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.max_size if self.is_full else self.index\n",
        "\n",
        "class RewardShaper:\n",
        "    def __init__(self):\n",
        "        self.previous_score = 0\n",
        "        self.previous_lives = 3\n",
        "        self.steps_since_last_shot = 0\n",
        "        self.last_known_position = None\n",
        "        self.consecutive_no_movement = 0\n",
        "\n",
        "    def calculate_shaped_reward(self, info, reward, action, position):\n",
        "        shaped_reward = reward  # Start with original reward\n",
        "\n",
        "        # Track score improvements - reduced multiplier\n",
        "        current_score = info.get('score', 0)\n",
        "        if current_score > self.previous_score:\n",
        "            score_improvement = current_score - self.previous_score\n",
        "            shaped_reward += 0.05 * score_improvement  # Reduced from 0.1 to 0.05\n",
        "            self.previous_score = current_score\n",
        "\n",
        "        # Penalize death - made less punitive\n",
        "        current_lives = info.get('lives', 3)\n",
        "        if current_lives < self.previous_lives:\n",
        "            shaped_reward -= 0.5  # Reduced from 1.0 to 0.5\n",
        "            self.previous_lives = current_lives\n",
        "\n",
        "        # Movement-based shaping - more lenient thresholds\n",
        "        if self.last_known_position is not None:\n",
        "            movement = abs(position - self.last_known_position)\n",
        "            if movement < 0.005:  # Reduced threshold from 0.01 to 0.005\n",
        "                self.consecutive_no_movement += 1\n",
        "                if self.consecutive_no_movement > 15:  # Increased from 10 to 15\n",
        "                    shaped_reward -= 0.05  # Reduced penalty from 0.1 to 0.05\n",
        "            else:\n",
        "                self.consecutive_no_movement = 0\n",
        "                # Smaller movement reward\n",
        "                shaped_reward += 0.005  # Reduced from 0.01 to 0.005\n",
        "\n",
        "        self.last_known_position = position\n",
        "\n",
        "        # Shooting behavior shaping - more balanced\n",
        "        if action == 1:  # Shoot action\n",
        "            self.steps_since_last_shot = 0\n",
        "            shaped_reward += 0.02  # Reduced from 0.05 to 0.02\n",
        "        else:\n",
        "            self.steps_since_last_shot += 1\n",
        "            if self.steps_since_last_shot > 30:  # Increased from 20 to 30\n",
        "                shaped_reward -= 0.02  # Reduced from 0.05 to 0.02\n",
        "\n",
        "        # Ensure reward doesn't go too negative\n",
        "        return max(-0.5, shaped_reward)  # Add a floor to prevent extreme negative rewards\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_shape, num_actions, learning_rate=0.00025):\n",
        "        self.state_shape = state_shape\n",
        "        self.num_actions = num_actions\n",
        "        self.model = self.create_q_model(state_shape, num_actions)\n",
        "        self.target_model = self.create_q_model(state_shape, num_actions)\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
        "        self.replay_buffer = ReplayBuffer(max_size=100000, state_shape=state_shape, num_actions=num_actions)\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_max = 1.0\n",
        "        self.epsilon_random_frames = 50000  # Initial random frames\n",
        "        self.epsilon_greedy_frames = 500000.0  # Total frames over which to decay epsilon\n",
        "\n",
        "    def create_q_model(self, input_shape, num_actions):\n",
        "        return tf.keras.Sequential([\n",
        "            layers.Input(shape=input_shape),\n",
        "            layers.Conv2D(32, kernel_size=8, strides=4, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "            layers.Conv2D(64, kernel_size=4, strides=2, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "            layers.Conv2D(64, kernel_size=3, strides=1, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(512, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "            layers.Dense(num_actions, activation=\"linear\")\n",
        "        ])\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.randint(self.num_actions)\n",
        "        state_tensor = tf.convert_to_tensor(state[np.newaxis, ...], dtype=tf.float32)\n",
        "        q_values = self.model(state_tensor, training=False)\n",
        "        return tf.argmax(q_values[0]).numpy()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, states, actions, rewards, next_states, dones):\n",
        "        future_rewards = self.target_model(next_states, training=False)\n",
        "        target_q_values = rewards + self.gamma * tf.reduce_max(future_rewards, axis=1) * (1 - dones)\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = self.model(states, training=True)\n",
        "            q_action = tf.reduce_sum(tf.one_hot(actions, self.num_actions) * q_values, axis=1)\n",
        "            loss = tf.keras.losses.Huber()(target_q_values, q_action)\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "def get_agent_position(state):\n",
        "    \"\"\"Extract the agent's position from the state\"\"\"\n",
        "    bottom_slice = state[-10:, :, -1]  # Look at the bottom portion of the last frame\n",
        "    agent_x = np.mean(np.where(bottom_slice > 0.5)[1]) if np.any(bottom_slice > 0.5) else None\n",
        "    if agent_x is not None:\n",
        "        return agent_x / state.shape[1]  # Normalize position\n",
        "    return 0.5  # Return center if agent not found\n",
        "\n",
        "def main():\n",
        "    # Paths for saving\n",
        "    model_dir = \"/content/drive/MyDrive/SpaceInvaders/models_2\"\n",
        "    plot_dir = \"/content/drive/MyDrive/SpaceInvaders/plots_2\"\n",
        "    log_dir = \"/content/drive/MyDrive/SpaceInvaders/logs_2\"\n",
        "    log_file_path = os.path.join(log_dir, \"training_log_2.txt\")\n",
        "\n",
        "    # Create all necessary directories\n",
        "    for directory in [model_dir, plot_dir, log_dir]:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Environment setup\n",
        "    gym.register_envs(ale_py)\n",
        "    env = gym.make(\"SpaceInvadersNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
        "    env = AtariPreprocessing(env, frame_skip=4, grayscale_obs=True, scale_obs=True)\n",
        "    env = FrameStackObservation(env, stack_size=4)\n",
        "\n",
        "    # Agent and reward shaper setup\n",
        "    state_shape = (84, 84, 4)\n",
        "    num_actions = env.action_space.n\n",
        "    agent = DQNAgent(state_shape, num_actions)\n",
        "    reward_shaper = RewardShaper()\n",
        "\n",
        "    # Training parameters\n",
        "    max_episodes = 1000  # Reduced from 2000\n",
        "    max_steps_per_episode = 3000\n",
        "    update_target_every = 1000\n",
        "    log_every = 1\n",
        "    episode_rewards = []\n",
        "    frame_count = 0\n",
        "    best_avg_reward = -float('inf')\n",
        "\n",
        "    with open(log_file_path, \"w\") as f:\n",
        "        f.write(\"Episode,Avg_Reward,Epsilon,Original_Reward,Shaped_Reward\\n\")\n",
        "\n",
        "    # Training loop\n",
        "    for episode in range(max_episodes):\n",
        "        state, info = env.reset()\n",
        "        state = np.array(state, dtype=np.float32) / 255.0\n",
        "        state = np.transpose(state, (1, 2, 0))\n",
        "        episode_reward = 0\n",
        "        episode_original_reward = 0\n",
        "        episode_steps = 0\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            frame_count += 1\n",
        "            episode_steps += 1\n",
        "\n",
        "            # Choose action\n",
        "            if frame_count < agent.epsilon_random_frames:\n",
        "                action = np.random.randint(num_actions)\n",
        "            else:\n",
        "                action = agent.get_action(state)\n",
        "\n",
        "            # Step environment\n",
        "            next_state, reward, done, _, info = env.step(action)\n",
        "            next_state = np.array(next_state, dtype=np.float32) / 255.0\n",
        "            next_state = np.transpose(next_state, (1, 2, 0))\n",
        "\n",
        "            # Shape the reward\n",
        "            agent_position = get_agent_position(state)\n",
        "            shaped_reward = reward_shaper.calculate_shaped_reward(info, reward, action, agent_position)\n",
        "\n",
        "            # Store original reward for logging\n",
        "            episode_original_reward += reward\n",
        "            episode_reward += shaped_reward\n",
        "\n",
        "            # Store in replay buffer\n",
        "            agent.replay_buffer.add(state, action, shaped_reward, next_state, done)\n",
        "            state = next_state\n",
        "\n",
        "            # Train\n",
        "            if len(agent.replay_buffer) >= 32:\n",
        "                states, actions, rewards, next_states, dones = agent.replay_buffer.sample(32)\n",
        "                agent.train_step(states, actions, rewards, next_states, dones)\n",
        "\n",
        "            # Update target model\n",
        "            if frame_count % update_target_every == 0:\n",
        "                agent.update_target_model()\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Episode-based epsilon decay\n",
        "        if frame_count >= agent.epsilon_random_frames:\n",
        "            decay_per_episode = (agent.epsilon_max - agent.epsilon_min) / (max_episodes * 0.7)  # Decay over first 70% of episodes\n",
        "            agent.epsilon = max(agent.epsilon_min, agent.epsilon - decay_per_episode)\n",
        "\n",
        "        episode_rewards.append(episode_reward)\n",
        "        avg_reward = np.mean(episode_rewards[-log_every:])\n",
        "\n",
        "        # Log both original and shaped rewards\n",
        "        with open(log_file_path, \"a\") as f:\n",
        "            f.write(f\"{episode + 1},{avg_reward:.2f},{agent.epsilon:.4f},{episode_original_reward:.2f},{episode_reward:.2f},{frame_count}\\n\")\n",
        "\n",
        "\n",
        "        # Save model conditionally\n",
        "        if (episode + 1) % 100 == 0 or avg_reward > best_avg_reward:\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "                print(f\"New best average reward: {best_avg_reward:.2f}. Saving best model.\")\n",
        "            agent.model.save(f\"{model_dir}/episode{episode + 1}_space_invaders_model.keras\")\n",
        "            print(f\"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}, Original Reward: {episode_original_reward:.2f}, \"\n",
        "              f\"Shaped Reward: {episode_reward:.2f}, Epsilon: {agent.epsilon:.4f}, Frame Count: {frame_count} \")\n",
        "\n",
        "    # Save final model\n",
        "    agent.model.save(f\"{model_dir}/final_space_invaders_model.keras\")\n",
        "    print(f\"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}, Original Reward: {episode_original_reward:.2f}, \"\n",
        "              f\"Shaped Reward: {episode_reward:.2f}, Epsilon: {agent.epsilon:.4f}, Frame Count: {frame_count} \")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        },
        "id": "dbJ0cVQ4nsPV",
        "outputId": "fe70b51d-bf89-4bd3-c000-ee794ada6615"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# File path to your log file\n",
        "filepath = '/content/drive/MyDrive/SpaceInvaders/logs_2/training_log_2.txt'\n",
        "\n",
        "# Read the data\n",
        "df = pd.read_csv(filepath, sep=',', index_col=False)\n",
        "\n",
        "# Directory to save the plot\n",
        "save_path = '/content/drive/MyDrive/SpaceInvaders/plots_2/training_log_plot_2.png'\n",
        "\n",
        "# Create a figure with two y-axes\n",
        "fig, ax1 = plt.subplots(figsize=(15, 7))\n",
        "\n",
        "# Plot Average Reward on the primary y-axis\n",
        "color1 = 'blue'\n",
        "ax1.set_xlabel('Episode')\n",
        "ax1.set_ylabel('Average Reward', color=color1)\n",
        "ax1.plot(df['Episode'], df['Avg_Reward'], color=color1, alpha=0.7, label='Average Reward')\n",
        "ax1.tick_params(axis='y', labelcolor=color1)\n",
        "\n",
        "# Create a secondary y-axis for Epsilon\n",
        "ax2 = ax1.twinx()\n",
        "color2 = 'red'\n",
        "ax2.set_ylabel('Epsilon', color=color2)\n",
        "ax2.plot(df['Episode'], df['Epsilon'], color=color2, alpha=0.7, label='Epsilon')\n",
        "ax2.tick_params(axis='y', labelcolor=color2)\n",
        "\n",
        "# Title and layout\n",
        "plt.title('Reinforcement Learning Training Log 2: Reward and Exploration Rate', fontsize=16)\n",
        "fig.tight_layout()\n",
        "\n",
        "# Add a legend\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "# Grid for better readability\n",
        "ax1.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Save the plot\n",
        "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"Plot saved at: {save_path}\")\n",
        "\n",
        "# Optional: Calculate and print some statistics\n",
        "print(f\"Total Episodes: {len(df)}\")\n",
        "print(f\"\\nReward Statistics:\")\n",
        "print(f\"Minimum Reward: {df['Avg_Reward'].min():.2f}\")\n",
        "print(f\"Maximum Reward: {df['Avg_Reward'].max():.2f}\")\n",
        "print(f\"Mean Reward: {df['Avg_Reward'].mean():.2f}\")\n",
        "print(f\"Median Reward: {df['Avg_Reward'].median():.2f}\")\n",
        "\n",
        "print(f\"\\nEpsilon Statistics:\")\n",
        "print(f\"Initial Epsilon: {df['Epsilon'].iloc[0]:.4f}\")\n",
        "print(f\"Final Epsilon: {df['Epsilon'].iloc[-1]:.4f}\")\n",
        "\n",
        "# Show the plot (optional, depends on your environment)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outcome Try 2\n",
        "\n",
        "Looking at both sets of results, here's a comparative analysis:\n",
        "\n",
        "1. Reward Stability:\n",
        "- Try 2 shows a more consistent baseline performance, with fewer extreme negative rewards (minimum 2.45 vs -27.55)\n",
        "- The maximum reward decreased (834.24 vs 1003.80), but this might actually be positive as it suggests less erratic behavior\n",
        "- Mean rewards are very similar (206.75 vs 207.78), indicating comparable overall performance\n",
        "- Median rewards are also close (178.39 vs 179.55), suggesting stable learning\n",
        "\n",
        "2. Learning Progression:\n",
        "- Try 2's epsilon decay over 70% of episodes (vs 30%) shows a more gradual exploration reduction\n",
        "- The lower final epsilon (0.01 vs 0.1) allowed for more exploitation of learned strategies\n",
        "- The reward variance appears more uniform across episodes in Try 2\n",
        "\n",
        "3. Key Improvements:\n",
        "- Elimination of severe negative rewards\n",
        "- More consistent performance across episodes\n",
        "- Better balance between exploration and exploitation\n",
        "\n",
        "4. Areas Still Needing Attention:\n",
        "- High variance still persists, though more uniformly distributed\n",
        "- No clear upward trend in average reward over time\n",
        "- Occasional performance spikes suggest potential for better consistency\n",
        "\n",
        "To further improve, we might consider:\n",
        "1. Implementing a more sophisticated reward normalization strategy\n",
        "2. Adding experience prioritization in the replay buffer\n",
        "3. Adjusting the network architecture or learning rate schedule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Try 3\n",
        "\n",
        "In this try we focused on\n",
        "\n",
        "    -  A more sophisticated reward strategy for shooting reward\n",
        "    - Strategic Gameplay Rewards:\n",
        "        Column clearing bonus (+0.2)\n",
        "        Smart shield usage (+0.05) with damage penalty (-0.1)\n",
        "        Bullet dodging reward (+0.1)\n",
        "        Wave survival bonus (+0.3)\n",
        "        Speed efficiency bonus (+0.2)\n",
        "        The Overall Shaping Strategy:\n",
        "         Maintains the core mechanics from try 2 (movement, basic shooting)\n",
        "        Adds strategic depth with situational bonuses\n",
        "        Introduces penalties for inefficient play\n",
        "        Creates a more complete reward hierarchy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03TxkxCmnCMQ",
        "outputId": "4afcc1c1-39d4-445b-d714-57f68bc4d35a"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import FrameStackObservation, AtariPreprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "import ale_py\n",
        "import matplotlib as plt\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size, state_shape, num_actions):\n",
        "        self.max_size = max_size\n",
        "        self.states = np.zeros((max_size, *state_shape), dtype=np.float32)\n",
        "        self.actions = np.zeros(max_size, dtype=np.int32)\n",
        "        self.rewards = np.zeros(max_size, dtype=np.float32)\n",
        "        self.next_states = np.zeros((max_size, *state_shape), dtype=np.float32)\n",
        "        self.dones = np.zeros(max_size, dtype=np.float32)\n",
        "        self.index = 0\n",
        "        self.is_full = False\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        idx = self.index % self.max_size\n",
        "        self.states[idx] = state\n",
        "        self.actions[idx] = action\n",
        "        self.rewards[idx] = reward\n",
        "        self.next_states[idx] = next_state\n",
        "        self.dones[idx] = done\n",
        "        self.index += 1\n",
        "        if self.index >= self.max_size:\n",
        "            self.is_full = True\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        max_index = self.max_size if self.is_full else self.index\n",
        "        indices = np.random.choice(max_index, batch_size, replace=False)\n",
        "        return (\n",
        "            self.states[indices],\n",
        "            self.actions[indices],\n",
        "            self.rewards[indices],\n",
        "            self.next_states[indices],\n",
        "            self.dones[indices]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.max_size if self.is_full else self.index\n",
        "\n",
        "class RewardShaper:\n",
        "    def __init__(self):\n",
        "        self.previous_score = 0\n",
        "        self.previous_lives = 3\n",
        "        self.steps_since_last_shot = 0\n",
        "        self.last_known_position = None\n",
        "        self.consecutive_no_movement = 0\n",
        "\n",
        "    def calculate_shaped_reward(self, info, reward, action, position):\n",
        "      shaped_reward = reward  # Start with original reward\n",
        "\n",
        "      # Track score improvements - reduced multiplier\n",
        "      current_score = info.get('score', 0)\n",
        "      if current_score > self.previous_score:\n",
        "          score_improvement = current_score - self.previous_score\n",
        "          shaped_reward += 0.05 * score_improvement  # Reduced from 0.1 to 0.05\n",
        "          self.previous_score = current_score\n",
        "\n",
        "      # Penalize death - made less punitive\n",
        "      current_lives = info.get('lives', 3)\n",
        "      if current_lives < self.previous_lives:\n",
        "          shaped_reward -= 0.5  # Reduced from 1.0 to 0.5\n",
        "          self.previous_lives = current_lives\n",
        "\n",
        "      # Movement-based shaping - more lenient thresholds\n",
        "      if self.last_known_position is not None:\n",
        "          movement = abs(position - self.last_known_position)\n",
        "          if movement < 0.005:  # Reduced threshold from 0.01 to 0.005\n",
        "              self.consecutive_no_movement += 1\n",
        "              if self.consecutive_no_movement > 15:  # Increased from 10 to 15\n",
        "                  shaped_reward -= 0.05  # Reduced penalty from 0.1 to 0.05\n",
        "          else:\n",
        "              self.consecutive_no_movement = 0\n",
        "              # Smaller movement reward\n",
        "              shaped_reward += 0.005  # Reduced from 0.01 to 0.005\n",
        "\n",
        "      self.last_known_position = position\n",
        "\n",
        "      # Shooting behavior shaping - more balanced\n",
        "      if action == 1:  # Shoot action\n",
        "          self.steps_since_last_shot = 0\n",
        "          if info.get('hit', False):  # Reward successful hits\n",
        "              shaped_reward += 0.1\n",
        "              if info.get('target_is_back_row', False):\n",
        "                  shaped_reward += 0.2  # Additional bonus for back-row targets\n",
        "              if info.get('target_is_ufo', False):\n",
        "                  shaped_reward += 0.2  # Additional bonus for UFO hits\n",
        "          else:\n",
        "              shaped_reward -= 0.05  # Penalize missed shots\n",
        "      else:\n",
        "          self.steps_since_last_shot += 1\n",
        "          if self.steps_since_last_shot > 30:  # Increased from 20 to 30\n",
        "              shaped_reward -= 0.02  # Reduced from 0.05 to 0.02\n",
        "\n",
        "      # Column clearing incentive\n",
        "      if info.get('cleared_column', False):\n",
        "          shaped_reward += 0.2  # Reward for clearing a column\n",
        "\n",
        "      # Shield usage shaping\n",
        "      if info.get('near_shield', False) and not info.get('destroying_shield', False):\n",
        "          shaped_reward += 0.05  # Incentive for smart shield use\n",
        "      if info.get('destroying_shield', False):\n",
        "          shaped_reward -= 0.1  # Penalty for damaging own shield\n",
        "\n",
        "      # Dodge enemy bullets\n",
        "      if info.get('dodged_bullet', False):\n",
        "          shaped_reward += 0.1  # Bonus for successful evasion\n",
        "\n",
        "      # Survival bonus for completing waves\n",
        "      if info.get('survived_wave', False):\n",
        "          shaped_reward += 0.3  # Bonus for wave survival\n",
        "\n",
        "      # Efficiency bonus for faster wave clear\n",
        "      if info.get('cleared_wave_faster_than_average', False):\n",
        "          shaped_reward += 0.2  # Efficiency reward\n",
        "\n",
        "      # Ensure reward doesn't go too negative\n",
        "      return max(-0.5, shaped_reward)  # Add a floor to prevent extreme negative rewards\n",
        "\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_shape, num_actions, learning_rate=0.00025):\n",
        "        self.state_shape = state_shape\n",
        "        self.num_actions = num_actions\n",
        "        self.model = self.create_q_model(state_shape, num_actions)\n",
        "        self.target_model = self.create_q_model(state_shape, num_actions)\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
        "        self.replay_buffer = ReplayBuffer(max_size=100000, state_shape=state_shape, num_actions=num_actions)\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_max = 1.0\n",
        "        self.epsilon_random_frames = 50000  # Initial random frames\n",
        "        self.epsilon_greedy_frames = 500000.0  # Total frames over which to decay epsilon\n",
        "\n",
        "    def create_q_model(self, input_shape, num_actions):\n",
        "        return tf.keras.Sequential([\n",
        "            layers.Input(shape=input_shape),\n",
        "            layers.Conv2D(32, kernel_size=8, strides=4, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "            layers.Conv2D(64, kernel_size=4, strides=2, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "            layers.Conv2D(64, kernel_size=3, strides=1, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(512, activation=\"relu\", kernel_initializer='he_uniform'),\n",
        "            layers.Dense(num_actions, activation=\"linear\")\n",
        "        ])\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.randint(self.num_actions)\n",
        "        state_tensor = tf.convert_to_tensor(state[np.newaxis, ...], dtype=tf.float32)\n",
        "        q_values = self.model(state_tensor, training=False)\n",
        "        return tf.argmax(q_values[0]).numpy()\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, states, actions, rewards, next_states, dones):\n",
        "        future_rewards = self.target_model(next_states, training=False)\n",
        "        target_q_values = rewards + self.gamma * tf.reduce_max(future_rewards, axis=1) * (1 - dones)\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = self.model(states, training=True)\n",
        "            q_action = tf.reduce_sum(tf.one_hot(actions, self.num_actions) * q_values, axis=1)\n",
        "            loss = tf.keras.losses.Huber()(target_q_values, q_action)\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "def get_agent_position(state):\n",
        "    \"\"\"Extract the agent's position from the state\"\"\"\n",
        "    bottom_slice = state[-10:, :, -1]  # Look at the bottom portion of the last frame\n",
        "    agent_x = np.mean(np.where(bottom_slice > 0.5)[1]) if np.any(bottom_slice > 0.5) else None\n",
        "    if agent_x is not None:\n",
        "        return agent_x / state.shape[1]  # Normalize position\n",
        "    return 0.5  # Return center if agent not found\n",
        "\n",
        "def main():\n",
        "    # Paths for saving\n",
        "    model_dir = \"/content/drive/MyDrive/SpaceInvaders/models_3\"\n",
        "    plot_dir = \"/content/drive/MyDrive/SpaceInvaders/plots_3\"\n",
        "    log_dir = \"/content/drive/MyDrive/SpaceInvaders/logs_3\"\n",
        "    log_file_path = os.path.join(log_dir, \"training_log_3.txt\")\n",
        "\n",
        "    # Create all necessary directories\n",
        "    for directory in [model_dir, plot_dir, log_dir]:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Environment setup\n",
        "    gym.register_envs(ale_py)\n",
        "    env = gym.make(\"SpaceInvadersNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
        "    env = AtariPreprocessing(env, frame_skip=4, grayscale_obs=True, scale_obs=True)\n",
        "    env = FrameStackObservation(env, stack_size=4)\n",
        "\n",
        "    # Agent and reward shaper setup\n",
        "    state_shape = (84, 84, 4)\n",
        "    num_actions = env.action_space.n\n",
        "    agent = DQNAgent(state_shape, num_actions)\n",
        "    reward_shaper = RewardShaper()\n",
        "\n",
        "    # Training parameters\n",
        "    max_episodes = 1000  # Reduced from 2000\n",
        "    max_steps_per_episode = 3000\n",
        "    update_target_every = 1000\n",
        "    log_every = 1\n",
        "    episode_rewards = []\n",
        "    frame_count = 0\n",
        "    best_avg_reward = -float('inf')\n",
        "\n",
        "    with open(log_file_path, \"w\") as f:\n",
        "        f.write(\"Episode,Avg_Reward,Epsilon,Original_Reward,Shaped_Reward\\n\")\n",
        "\n",
        "    # Training loop\n",
        "    for episode in range(max_episodes):\n",
        "        state, info = env.reset()\n",
        "        state = np.array(state, dtype=np.float32) / 255.0\n",
        "        state = np.transpose(state, (1, 2, 0))\n",
        "        episode_reward = 0\n",
        "        episode_original_reward = 0\n",
        "        episode_steps = 0\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            frame_count += 1\n",
        "            episode_steps += 1\n",
        "\n",
        "            # Choose action\n",
        "            if frame_count < agent.epsilon_random_frames:\n",
        "                action = np.random.randint(num_actions)\n",
        "            else:\n",
        "                action = agent.get_action(state)\n",
        "\n",
        "            # Step environment\n",
        "            next_state, reward, done, _, info = env.step(action)\n",
        "            next_state = np.array(next_state, dtype=np.float32) / 255.0\n",
        "            next_state = np.transpose(next_state, (1, 2, 0))\n",
        "\n",
        "            # Shape the reward\n",
        "            agent_position = get_agent_position(state)\n",
        "            shaped_reward = reward_shaper.calculate_shaped_reward(info, reward, action, agent_position)\n",
        "\n",
        "            # Store original reward for logging\n",
        "            episode_original_reward += reward\n",
        "            episode_reward += shaped_reward\n",
        "\n",
        "            # Store in replay buffer\n",
        "            agent.replay_buffer.add(state, action, shaped_reward, next_state, done)\n",
        "            state = next_state\n",
        "\n",
        "            # Train\n",
        "            if len(agent.replay_buffer) >= 32:\n",
        "                states, actions, rewards, next_states, dones = agent.replay_buffer.sample(32)\n",
        "                agent.train_step(states, actions, rewards, next_states, dones)\n",
        "\n",
        "            # Update target model\n",
        "            if frame_count % update_target_every == 0:\n",
        "                agent.update_target_model()\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Episode-based epsilon decay\n",
        "        if frame_count >= agent.epsilon_random_frames:\n",
        "            decay_per_episode = (agent.epsilon_max - agent.epsilon_min) / (max_episodes * 0.7)  # Decay over first 70% of episodes\n",
        "            agent.epsilon = max(agent.epsilon_min, agent.epsilon - decay_per_episode)\n",
        "\n",
        "        episode_rewards.append(episode_reward)\n",
        "        avg_reward = np.mean(episode_rewards[-log_every:])\n",
        "\n",
        "        # Log both original and shaped rewards\n",
        "        with open(log_file_path, \"a\") as f:\n",
        "            f.write(f\"{episode + 1},{avg_reward:.2f},{agent.epsilon:.4f},{episode_original_reward:.2f},{episode_reward:.2f},{frame_count}\\n\")\n",
        "\n",
        "\n",
        "        # Save model conditionally\n",
        "        if (episode + 1) % 100 == 0 or avg_reward > best_avg_reward:\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "                print(f\"New best average reward: {best_avg_reward:.2f}. Saving best model.\")\n",
        "            agent.model.save(f\"{model_dir}/episode{episode + 1}_space_invaders_model.keras\")\n",
        "            print(f\"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}, Original Reward: {episode_original_reward:.2f}, \"\n",
        "              f\"Shaped Reward: {episode_reward:.2f}, Epsilon: {agent.epsilon:.4f}, Frame Count: {frame_count} \")\n",
        "\n",
        "    # Save final model\n",
        "    agent.model.save(f\"{model_dir}/final_space_invaders_model.keras\")\n",
        "    print(f\"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}, Original Reward: {episode_original_reward:.2f}, \"\n",
        "              f\"Shaped Reward: {episode_reward:.2f}, Epsilon: {agent.epsilon:.4f}, Frame Count: {frame_count} \")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        },
        "id": "tz4Iuw8-5Hvt",
        "outputId": "422af33d-e58a-41f3-d7c9-d55a67280030"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# File path to your log file\n",
        "filepath = '/content/drive/MyDrive/SpaceInvaders/logs_3/training_log_3.txt'\n",
        "\n",
        "# Read the data\n",
        "df = pd.read_csv(filepath, sep=',', index_col=False)\n",
        "\n",
        "# Directory to save the plot\n",
        "save_path = '/content/drive/MyDrive/SpaceInvaders/plots_3/training_log_plot_3.png'\n",
        "\n",
        "# Create a figure with two y-axes\n",
        "fig, ax1 = plt.subplots(figsize=(15, 7))\n",
        "\n",
        "# Plot Average Reward on the primary y-axis\n",
        "color1 = 'blue'\n",
        "ax1.set_xlabel('Episode')\n",
        "ax1.set_ylabel('Average Reward', color=color1)\n",
        "ax1.plot(df['Episode'], df['Avg_Reward'], color=color1, alpha=0.7, label='Average Reward')\n",
        "ax1.tick_params(axis='y', labelcolor=color1)\n",
        "\n",
        "# Create a secondary y-axis for Epsilon\n",
        "ax2 = ax1.twinx()\n",
        "color2 = 'red'\n",
        "ax2.set_ylabel('Epsilon', color=color2)\n",
        "ax2.plot(df['Episode'], df['Epsilon'], color=color2, alpha=0.7, label='Epsilon')\n",
        "ax2.tick_params(axis='y', labelcolor=color2)\n",
        "\n",
        "# Title and layout\n",
        "plt.title('Reinforcement Learning Training Log 3: Reward and Exploration Rate', fontsize=16)\n",
        "fig.tight_layout()\n",
        "\n",
        "# Add a legend\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "# Grid for better readability\n",
        "ax1.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Save the plot\n",
        "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"Plot saved at: {save_path}\")\n",
        "\n",
        "# Optional: Calculate and print some statistics\n",
        "print(f\"Total Episodes: {len(df)}\")\n",
        "print(f\"\\nReward Statistics:\")\n",
        "print(f\"Minimum Reward: {df['Avg_Reward'].min():.2f}\")\n",
        "print(f\"Maximum Reward: {df['Avg_Reward'].max():.2f}\")\n",
        "print(f\"Mean Reward: {df['Avg_Reward'].mean():.2f}\")\n",
        "print(f\"Median Reward: {df['Avg_Reward'].median():.2f}\")\n",
        "\n",
        "print(f\"\\nEpsilon Statistics:\")\n",
        "print(f\"Initial Epsilon: {df['Epsilon'].iloc[0]:.4f}\")\n",
        "print(f\"Final Epsilon: {df['Epsilon'].iloc[-1]:.4f}\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outcome Try 3\n",
        "\n",
        "\n",
        "Try 2:              \n",
        "Min: 2.45            \n",
        "Max: 834.24          \n",
        "Mean: 206.75         \n",
        "Median: 178.39       \n",
        "\n",
        "\n",
        "  Try 3:\n",
        "  Min: -17.60                         \n",
        "  Max: 631.96                     \n",
        "  Mean: 150.79                    \n",
        "  Median: 126.97            \n",
        "\n",
        "#### Key Observations:\n",
        "\n",
        "Overall lower performance (mean dropped by ~27%)\n",
        "\n",
        "Return of negative rewards (-17.60)\n",
        "\n",
        "Lower maximum rewards (drop of ~200 points)\n",
        "\n",
        "More compressed reward range\n",
        "\n",
        "Similar variance pattern but at a lower baseline\n",
        "\n",
        "\n",
        "Likely Issues with New Reward Structure:\n",
        "\n",
        "The complex reward system may be creating competing objectives\n",
        "\n",
        "Larger reward values (0.1-0.3) might be overwhelming the base game rewards\n",
        "\n",
        "Multiple simultaneous rewards could be creating noise in the learning signal\n",
        "\n",
        "The agent might be struggling to correlate delayed rewards with actions\n",
        "\n",
        "#### Suggested Improvements if there were time:\n",
        "\n",
        "\n",
        "Scale back reward complexity\n",
        "\n",
        "Normalize reward values to be more consistent\n",
        "\n",
        "Focus on fewer, clearer reward signals\n",
        "\n",
        "Consider reducing the magnitude gap between different rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Space Invaders DQN Training Analysis - 3 Rounds\n",
        "\n",
        "## Performance Metrics Comparison\n",
        "\n",
        "| Metric | Try 1 | Try 2 | Try 3 |\n",
        "|--------|-------|-------|-------|\n",
        "| Minimum Reward | -27.55 | 2.45 | -17.60 |\n",
        "| Maximum Reward | 1003.80 | 834.24 | 631.96 |\n",
        "| Mean Reward | 207.78 | 206.75 | 150.79 |\n",
        "| Median Reward | 179.55 | 178.39 | 126.97 |\n",
        "| Final Epsilon | 0.1000 | 0.0100 | 0.0100 |\n",
        "\n",
        "## Key Findings Per Round\n",
        "\n",
        "### Try 1 (Basic Implementation)\n",
        "- Highest maximum reward but also largest negative rewards\n",
        "- High variance in performance\n",
        "- Conservative exploration (epsilon = 0.1)\n",
        "- Showed potential for high scores but lacked consistency\n",
        "\n",
        "### Try 2 (Balanced Approach)\n",
        "- Eliminated negative rewards\n",
        "- More stable performance\n",
        "- Better exploration-exploitation balance (epsilon = 0.01)\n",
        "- Most consistent mean/median performance\n",
        "- Best overall balance of stability and performance\n",
        "\n",
        "### Try 3 (Complex Reward Shaping)\n",
        "- Added sophisticated reward mechanisms\n",
        "- Lower overall performance\n",
        "- Reintroduction of negative rewards\n",
        "- More compressed reward range\n",
        "- Complex reward structure may have hindered learning\n",
        "\n",
        "## Learning Trends\n",
        "1. Epsilon decay became more gradual across attempts\n",
        "2. Performance consistency improved from Try 1 to Try 2\n",
        "3. Added complexity in Try 3 led to decreased performance\n",
        "4. Try 2 achieved the best balance of exploration and stability\n",
        "\n",
        "## Final Verdict\n",
        "Try 2 emerged as the most successful implementation, suggesting that moderate reward shaping with balanced exploration-exploitation tradeoff produces better results than either basic or complex approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import FrameStackObservation, AtariPreprocessing\n",
        "import numpy as np\n",
        "import ale_py\n",
        "import time\n",
        "import cv2\n",
        "\n",
        "def load_model(model_path):\n",
        "    \"\"\"Load a saved model\"\"\"\n",
        "    return tf.keras.models.load_model(model_path)\n",
        "\n",
        "def get_agent_position(state):\n",
        "    \"\"\"Extract the agent's position from the state\"\"\"\n",
        "    bottom_slice = state[-10:, :, -1]\n",
        "    agent_x = np.mean(np.where(bottom_slice > 0.5)[1]) if np.any(bottom_slice > 0.5) else None\n",
        "    if agent_x is not None:\n",
        "        return agent_x / state.shape[1]\n",
        "    return 0.5\n",
        "\n",
        "def evaluate_model(model_path, num_episodes=5, render=True):\n",
        "    \"\"\"Evaluate a model's performance\"\"\"\n",
        "    # Environment setup\n",
        "    env = gym.make(\"SpaceInvadersNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
        "    env = AtariPreprocessing(env, frame_skip=4, grayscale_obs=True, scale_obs=True)\n",
        "    env = FrameStackObservation(env, stack_size=4)\n",
        "    \n",
        "    model = load_model(model_path)\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, info = env.reset()\n",
        "        state = np.array(state, dtype=np.float32) / 255.0\n",
        "        state = np.transpose(state, (1, 2, 0))\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done and steps < 3000:  # Same step limit as training\n",
        "            # Get action from model\n",
        "            state_tensor = tf.convert_to_tensor(state[np.newaxis, ...], dtype=tf.float32)\n",
        "            q_values = model(state_tensor, training=False)\n",
        "            action = tf.argmax(q_values[0]).numpy()\n",
        "\n",
        "            # Take action\n",
        "            next_state, reward, done, _, info = env.step(action)\n",
        "            next_state = np.array(next_state, dtype=np.float32) / 255.0\n",
        "            next_state = np.transpose(next_state, (1, 2, 0))\n",
        "            \n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "            if render:\n",
        "                # Get the RGB frame from the environment\n",
        "                frame = env.render()\n",
        "                \n",
        "                # Display the frame using cv2\n",
        "                cv2.imshow('Space Invaders', cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "                cv2.waitKey(20)  # 20ms delay between frames\n",
        "\n",
        "        episode_rewards.append(episode_reward)\n",
        "        print(f\"Episode {episode + 1} Reward: {episode_reward}\")\n",
        "\n",
        "    if render:\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "    return np.mean(episode_rewards)\n",
        "\n",
        "def main():\n",
        "    # Paths to your saved models\n",
        "    model_paths = [\n",
        "        \"/models/models_1/final_space_invaders_model_1.keras\",\n",
        "        \"/models/models_2/final_space_invaders_model_2.keras\",\n",
        "        \"/models/models_3/final_space_invaders_model_3.keras\"\n",
        "    ]\n",
        "\n",
        "    print(\"Evaluating all models...\")\n",
        "    for i, model_path in enumerate(model_paths, 1):\n",
        "        print(f\"\\nEvaluating Model {i}\")\n",
        "        try:\n",
        "            avg_reward = evaluate_model(model_path, num_episodes=5, render=True)\n",
        "            print(f\"Model {i} Average Reward over 5 episodes: {avg_reward}\")\n",
        "            time.sleep(2)  # Pause between models\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating model {i}: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPQUtsOd7K4iuxFoZJI9sSN",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "mount_file_id": "1ZTCnjUY4kFipOa3lvhCugpYKcAAVKEyT",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deep_learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
